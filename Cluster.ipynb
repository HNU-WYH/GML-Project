{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzFXzJMYMK7H"
      },
      "source": [
        "# **ENVIRONMENT SETUP**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_j64Nt4MLah4"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctT2HVbfL257"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/CS4350/project/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wCEW-VWFL72r"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.7.1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3h_0tmhMAK3"
      },
      "outputs": [],
      "source": [
        "%pip install -q pyg-lib -f https://data.pyg.org/whl/torch-2.6.0+cu124.html\n",
        "%pip install -q torch torchvision torchaudio\n",
        "%pip install -q torch-scatter -f https://data.pyg.org/whl/torch-2.6.0+cu124.html  # prevents the wheel from taking forever to build\n",
        "%pip install -q torch-sparse -f https://data.pyg.org/whl/torch-2.6.0+cu124.html   # prevents the wheel from taking forever to build\n",
        "%pip install -q torch-cluster -f https://data.pyg.org/whl/torch-2.6.0+cu124.html  # prevents the wheel from taking forever to build\n",
        "%pip install -q torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOdUh3BUMD7B"
      },
      "outputs": [],
      "source": [
        "# run this only if you don't already have a copy of the built numml wheel on google drive (~7 mins)\n",
        "# build the wheel just once and then store in google drive /content/drive/MyDrive/CS4350/project/wheels\n",
        "!git clone https://github.com/nicknytko/numml.git\n",
        "%cd numml\n",
        "!pip install --upgrade build\n",
        "!python -m build --wheel\n",
        "!mkdir -p /content/drive/MyDrive/CS4350/project/wheels\n",
        "!mv dist/*.whl /content/drive/MyDrive/CS4350/project/wheels\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Q4AB7kqWUqI"
      },
      "outputs": [],
      "source": [
        "# once the .whl file is in the drive, just run this cell to install numml within seconds\n",
        "!pip install \\\n",
        "  /content/drive/MyDrive/CS4350/project/wheels/numml-0.1.0-cp311-cp311-linux_x86_64.whl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ToKfNRvRkxGc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/jylow/Documents/Y3S2/CS4350/GML-Project\n"
          ]
        }
      ],
      "source": [
        "%cd ../Documents/Y3S2/CS4350/GML-Project\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lqhvjS5eMJnf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import datetime\n",
        "import pprint\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import scipy\n",
        "import torch\n",
        "import torch_geometric\n",
        "import torch.nn as nn\n",
        "import torch_geometric.nn as pyg\n",
        "from torch_geometric.nn import aggr\n",
        "from torch_geometric.utils import to_scipy_sparse_matrix\n",
        "from torch_geometric.loader import NeighborLoader\n",
        "from torch_geometric.data import Batch\n",
        "from scipy.sparse import tril, coo_matrix\n",
        "\n",
        "from apps.data import get_dataloader, graph_to_matrix, matrix_to_graph, FolderDataset\n",
        "from neuralif.utils import (\n",
        "    count_parameters, save_dict_to_file,\n",
        "    condition_number, eigenval_distribution, gershgorin_norm,\n",
        "    TwoHop\n",
        ")\n",
        "from neuralif.logger import TrainResults, TestResults\n",
        "from neuralif.loss import loss\n",
        "\n",
        "from krylov.cg import preconditioned_conjugate_gradient\n",
        "from krylov.gmres import gmres\n",
        "\n",
        "from numml.sparse import SparseCSRTensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "mTrX6PeHhzdH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: nvcc\n",
            "Apple clang version 17.0.0 (clang-1700.0.13.3)\n",
            "Target: arm64-apple-darwin24.3.0\n",
            "Thread model: posix\n",
            "InstalledDir: /Library/Developer/CommandLineTools/usr/bin\n",
            "2.7.1 None\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version\n",
        "!gcc --version\n",
        "!python -c \"import torch; print(torch.__version__, torch.version.cuda)\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ecrpeZYaQSbH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(f\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zwz-aA6XQt5j"
      },
      "source": [
        "# **DATASET CREATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "aBjOJC57Qxhc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating 1000 samples for the train dataset.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [50:44<00:00,  3.04s/it]\n",
            "/Users/jylow/Documents/Y3S2/CS4350/GML-Project/apps/synthetic.py:87: UserWarning: rs must be set for test and val to avoid overlap\n",
            "  warnings.warn('rs must be set for test and val to avoid overlap')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating 10 samples for the val dataset.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:18<00:00,  1.86s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating 100 samples for the test dataset.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [03:03<00:00,  1.83s/it]\n"
          ]
        }
      ],
      "source": [
        "from apps.synthetic import create_dataset\n",
        "n = 10_000\n",
        "alpha=10e-4\n",
        "\n",
        "create_dataset(n, 1000, alpha=alpha, mode='train', rs=0, graph=True, solution=True)\n",
        "create_dataset(n, 10, alpha=alpha, mode='val', rs=10000, graph=True)\n",
        "create_dataset(n, 100, alpha=alpha, mode='test', rs=103600, graph=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4bMM1n0TOVNW"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"name\": \"experiment_1\",\n",
        "    \"save\": True,\n",
        "    \"seed\": 42,\n",
        "    \"n\": 10000,\n",
        "    \"batch_size\": 1,\n",
        "    \"num_epochs\": 10000,\n",
        "    \"dataset\": \"random\",\n",
        "    \"loss\": None,\n",
        "    \"gradient_clipping\": 1.0,\n",
        "    \"regularizer\": 0.0,\n",
        "    \"scheduler\": False,\n",
        "    \"model\": \"neuralif\",\n",
        "    \"normalize\": False,\n",
        "    \"latent_size\": 8,\n",
        "    \"message_passing_steps\": 3,\n",
        "    \"decode_nodes\": False,\n",
        "    \"normalize_diag\": False,\n",
        "    \"aggregate\": [\"mean\", \"sum\"],\n",
        "    \"activation\": \"relu\",\n",
        "    \"skip_connections\": True,\n",
        "    \"augment_nodes\": False,\n",
        "    \"global_features\": 0,\n",
        "    \"edge_features\": 1,\n",
        "    \"graph_norm\": False,\n",
        "    \"two_hop\": False,\n",
        "    \"num_neighbors\": [15, 10],  # number of neighbours to sample in each hop (GraphSAGE sampling)\n",
        "    \"num_clusters\": 10,  # Number of clusters to partition each graph into\n",
        "    \"clusters_per_batch\": 3,  # Number of clusters to sample per batch\n",
        "    \"cluster_method\": \"metis\"  # Options: 'metis', 'random', 'kmeans'\n",
        "}\n",
        "\n",
        "# Prepare output folder\n",
        "if config[\"name\"]:\n",
        "    folder = f\"results/{config['name']}\"\n",
        "else:\n",
        "    folder = datetime.datetime.now().strftime(\"results/%Y-%m-%d_%H-%M-%S\")\n",
        "if config[\"save\"]:\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "    save_dict_to_file(config, os.path.join(folder, \"config.json\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaOuG4eZ-mKl"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install libmetis-dev\n",
        "!pip install metis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zrEdDUMu9ML2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data, Batch\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.utils import subgraph\n",
        "import torch_geometric\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "import metis  # pip install metis\n",
        "import time\n",
        "import os\n",
        "\n",
        "class ClusterGCNSampler:\n",
        "    def __init__(self, num_clusters, cluster_method='metis'):\n",
        "        self.num_clusters = num_clusters\n",
        "        self.cluster_method = cluster_method\n",
        "        self.node_clusters = {}  # Cache for node cluster assignments\n",
        "\n",
        "    def partition_graph(self, data):\n",
        "        edge_index = data.edge_index\n",
        "        num_nodes = data.num_nodes\n",
        "\n",
        "        # Create adjacency list for clustering\n",
        "        if self.cluster_method == 'metis':\n",
        "            # Convert to format expected by METIS\n",
        "            adjacency_list = [[] for _ in range(num_nodes)]\n",
        "            for i in range(edge_index.size(1)):\n",
        "                src, dst = edge_index[0, i].item(), edge_index[1, i].item()\n",
        "                if src != dst:  # Avoid self-loops for clustering\n",
        "                    adjacency_list[src].append(dst)\n",
        "                    adjacency_list[dst].append(src)\n",
        "\n",
        "            try:\n",
        "                # Use METIS for graph partitioning\n",
        "                _, node_clusters = metis.part_graph(adjacency_list, self.num_clusters)\n",
        "                node_clusters = torch.tensor(node_clusters, dtype=torch.long)\n",
        "            except:\n",
        "                # Fallback to random partitioning if METIS fails\n",
        "                print(\"metis failed\")\n",
        "                node_clusters = torch.randint(0, self.num_clusters, (num_nodes,))\n",
        "\n",
        "        return node_clusters\n",
        "\n",
        "    def sample_clusters(self, data, clusters_per_batch):\n",
        "        \"\"\"\n",
        "        Sample a subset of clusters and return the induced subgraph\n",
        "        \"\"\"\n",
        "        # Get or compute node cluster assignments\n",
        "        graph_id = id(data)  # Use object id as cache key\n",
        "        if graph_id not in self.node_clusters:\n",
        "            self.node_clusters[graph_id] = self.partition_graph(data)\n",
        "\n",
        "        node_clusters = self.node_clusters[graph_id]\n",
        "\n",
        "        # Randomly sample clusters for this batch\n",
        "        available_clusters = torch.unique(node_clusters)\n",
        "        if len(available_clusters) <= clusters_per_batch:\n",
        "            selected_clusters = available_clusters\n",
        "        else:\n",
        "            selected_clusters = available_clusters[torch.randperm(len(available_clusters))[:clusters_per_batch]]\n",
        "\n",
        "        # Get nodes belonging to selected clusters\n",
        "        mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
        "        for cluster_id in selected_clusters:\n",
        "            mask |= (node_clusters == cluster_id)\n",
        "\n",
        "        selected_nodes = torch.where(mask)[0]\n",
        "\n",
        "        # Extract subgraph\n",
        "        edge_index, edge_attr = subgraph(\n",
        "            selected_nodes,\n",
        "            data.edge_index,\n",
        "            edge_attr=data.edge_attr if hasattr(data, 'edge_attr') else None,\n",
        "            relabel_nodes=True,\n",
        "            num_nodes=data.num_nodes\n",
        "        )\n",
        "\n",
        "        # Create new data object for the subgraph\n",
        "        subgraph_data = Data(\n",
        "            edge_index=edge_index,\n",
        "            num_nodes=len(selected_nodes)\n",
        "        )\n",
        "\n",
        "        # Copy relevant attributes\n",
        "        if hasattr(data, 'x') and data.x is not None:\n",
        "            subgraph_data.x = data.x[selected_nodes]\n",
        "\n",
        "        if hasattr(data, 'edge_attr') and data.edge_attr is not None:\n",
        "            subgraph_data.edge_attr = edge_attr\n",
        "\n",
        "        # For your preconditioner application, you'll need to handle the matrix\n",
        "        if hasattr(data, 'matrix'):\n",
        "            # Extract the submatrix corresponding to selected nodes\n",
        "            subgraph_data.matrix = data.matrix[selected_nodes][:, selected_nodes]\n",
        "\n",
        "        # Store mapping for reconstruction if needed\n",
        "        subgraph_data.original_nodes = selected_nodes\n",
        "        subgraph_data.node_mapping = {new_idx: old_idx.item()\n",
        "                                    for new_idx, old_idx in enumerate(selected_nodes)}\n",
        "\n",
        "        return subgraph_data\n",
        "\n",
        "class ClusterGCNDataLoader:\n",
        "    \"\"\"\n",
        "    Custom DataLoader that uses ClusterGCN sampling\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset, batch_size, num_clusters, clusters_per_batch,\n",
        "                 cluster_method='metis', shuffle=True):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.sampler = ClusterGCNSampler(num_clusters, cluster_method)\n",
        "        self.clusters_per_batch = clusters_per_batch\n",
        "\n",
        "    def __iter__(self):\n",
        "        indices = list(range(len(self.dataset)))\n",
        "        if self.shuffle:\n",
        "            torch.randperm(len(indices))\n",
        "\n",
        "        for i in range(0, len(indices), self.batch_size):\n",
        "            batch_indices = indices[i:i + self.batch_size]\n",
        "            batch_data = []\n",
        "\n",
        "            for idx in batch_indices:\n",
        "                data = self.dataset[idx]\n",
        "                # Sample clusters for this graph\n",
        "                subgraph_data = self.sampler.sample_clusters(data, self.clusters_per_batch)\n",
        "                batch_data.append(subgraph_data)\n",
        "\n",
        "            if len(batch_data) == 1:\n",
        "                yield batch_data[0]\n",
        "            else:\n",
        "                yield Batch.from_data_list(batch_data)\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.dataset) + self.batch_size - 1) // self.batch_size\n",
        "\n",
        "# Modified training function for your code\n",
        "def get_cluster_dataloader(dataset, n, size, spd=True, mode=\"train\",\n",
        "                          num_clusters=10, clusters_per_batch=2):\n",
        "    \"\"\"\n",
        "    Modified version of your get_dataloader function that uses ClusterGCN\n",
        "    \"\"\"\n",
        "    # Get your original dataset (replace with your actual dataset loading logic)\n",
        "    # dataset = get_dataset(dataset_name, n, spd, mode)  # Your existing function\n",
        "    data = FolderDataset(f\"./dataset/{mode}/\", n, size=size, graph=True)\n",
        "\n",
        "    # if dataset == \"random\":\n",
        "    #     data = FolderDataset(f\"./dataset/{mode}/\", n, size=size, graph=graph)\n",
        "\n",
        "    # else:\n",
        "    #     raise NotImplementedError(\"Dataset not implemented, Available: random\")\n",
        "\n",
        "    # # Data Loaders\n",
        "    # if mode == \"train\":\n",
        "    #     dataloader = DataLoader(data, batch_size=batch_size, shuffle=True)\n",
        "    # else:\n",
        "    #     dataloader = DataLoader(data, batch_size=1, shuffle=False)\n",
        "\n",
        "    # Wrap with ClusterGCN loader\n",
        "    return ClusterGCNDataLoader(\n",
        "        data,\n",
        "        batch_size=size,\n",
        "        num_clusters=num_clusters,\n",
        "        clusters_per_batch=clusters_per_batch,\n",
        "        cluster_method='metis',\n",
        "        shuffle=(mode == \"train\")\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "33unmKPgQAC1"
      },
      "outputs": [],
      "source": [
        "class GraphNet(nn.Module):\n",
        "    # Follows roughly the outline of torch_geometric.nn.MessagePassing()\n",
        "    # As shown in https://github.com/deepmind/graph_nets\n",
        "    # Here is a helpful python implementation:\n",
        "    # https://github.com/NVIDIA/GraphQSat/blob/main/gqsat/models.py\n",
        "    # Also allows multirgaph GNN via edge_2_features\n",
        "    def __init__(self, node_features, edge_features, global_features=0, hidden_size=0,\n",
        "                 aggregate=\"mean\", activation=\"relu\", skip_connection=False, edge_features_out=None):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # different aggregation functions\n",
        "        if aggregate == \"sum\":\n",
        "            self.aggregate = aggr.SumAggregation()\n",
        "        elif aggregate == \"mean\":\n",
        "            self.aggregate = aggr.MeanAggregation()\n",
        "        elif aggregate == \"max\":\n",
        "            self.aggregate = aggr.MaxAggregation()\n",
        "        elif aggregate == \"softmax\":\n",
        "            self.aggregate = aggr.SoftmaxAggregation(learn=True)\n",
        "        else:\n",
        "            raise NotImplementedError(f\"Aggregation '{aggregate}' not implemented\")\n",
        "\n",
        "        self.global_aggregate = aggr.MeanAggregation()\n",
        "\n",
        "        add_edge_fs = 1 if skip_connection else 0\n",
        "        edge_features_out = edge_features if edge_features_out is None else edge_features_out\n",
        "\n",
        "        # Graph Net Blocks (see https://arxiv.org/pdf/1806.01261.pdf)\n",
        "        self.edge_block = MLP([global_features + (edge_features + add_edge_fs) + (2 * node_features),\n",
        "                               hidden_size,\n",
        "                               edge_features_out],\n",
        "                              activation=activation)\n",
        "\n",
        "        self.node_block = MLP([global_features + edge_features_out + node_features,\n",
        "                               hidden_size,\n",
        "                               node_features],\n",
        "                              activation=activation)\n",
        "\n",
        "        # optional set of blocks for global GNN\n",
        "        self.global_block = None\n",
        "        if global_features > 0:\n",
        "            self.global_block = MLP([edge_features_out + node_features + global_features,\n",
        "                                     hidden_size,\n",
        "                                     global_features],\n",
        "                                    activation=activation)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr, g=None):\n",
        "        row, col = edge_index\n",
        "\n",
        "        if self.global_block is not None:\n",
        "            assert g is not None, \"Need global features for global block\"\n",
        "\n",
        "            # run the edge update and aggregate features\n",
        "            edge_embedding = self.edge_block(torch.cat([torch.ones(x[row].shape[0], 1, device=x.device) * g,\n",
        "                                                        x[row], x[col], edge_attr], dim=1))\n",
        "            aggregation = self.aggregate(edge_embedding, row)\n",
        "\n",
        "\n",
        "            agg_features = torch.cat([torch.ones(x.shape[0], 1, device=x.device) * g, x, aggregation], dim=1)\n",
        "            node_embeddings = self.node_block(agg_features)\n",
        "\n",
        "            # aggregate over all edges and nodes (always mean)\n",
        "            mp_global_aggr = g\n",
        "            edge_aggregation_global = self.global_aggregate(edge_embedding)\n",
        "            node_aggregation_global = self.global_aggregate(node_embeddings)\n",
        "\n",
        "            # compute the new global embedding\n",
        "            # the old global feature is part of mp_global_aggr\n",
        "            global_embeddings = self.global_block(torch.cat([node_aggregation_global,\n",
        "                                                             edge_aggregation_global,\n",
        "                                                             mp_global_aggr], dim=1))\n",
        "\n",
        "            return edge_embedding, node_embeddings, global_embeddings\n",
        "\n",
        "        else:\n",
        "            # update edge features and aggregate\n",
        "            edge_embedding = self.edge_block(torch.cat([x[row], x[col], edge_attr], dim=1))\n",
        "            aggregation = self.aggregate(edge_embedding, row)\n",
        "            agg_features = torch.cat([x, aggregation], dim=1)\n",
        "            # update node features\n",
        "            node_embeddings = self.node_block(agg_features)\n",
        "            return edge_embedding, node_embeddings, None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "HZBjB3GUPtrV"
      },
      "outputs": [],
      "source": [
        "class NeuralIF(nn.Module):\n",
        "    # Neural Incomplete factorization\n",
        "    def __init__(self, drop_tol=0, **kwargs) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.global_features = kwargs[\"global_features\"]\n",
        "        self.latent_size = kwargs[\"latent_size\"]\n",
        "        # node features are augmented with local degree profile\n",
        "        self.augment_node_features = kwargs[\"augment_nodes\"]\n",
        "\n",
        "        num_node_features = 8 if self.augment_node_features else 1\n",
        "        message_passing_steps = kwargs[\"message_passing_steps\"]\n",
        "\n",
        "        # edge feature representation in the latent layers\n",
        "        edge_features = kwargs.get(\"edge_features\", 1)\n",
        "\n",
        "        self.skip_connections = kwargs[\"skip_connections\"]\n",
        "\n",
        "        self.mps = torch.nn.ModuleList()\n",
        "        for l in range(message_passing_steps):\n",
        "            # skip connections are added to all layers except the first one\n",
        "            self.mps.append(MP_Block(skip_connections=self.skip_connections,\n",
        "                                     first=l==0,\n",
        "                                     last=l==(message_passing_steps-1),\n",
        "                                     edge_features=edge_features,\n",
        "                                     node_features=num_node_features,\n",
        "                                     global_features=self.global_features,\n",
        "                                     hidden_size=self.latent_size,\n",
        "                                     activation=kwargs[\"activation\"],\n",
        "                                     aggregate=kwargs[\"aggregate\"]))\n",
        "\n",
        "        # node decodings\n",
        "        self.node_decoder = MLP([num_node_features, self.latent_size, 1]) if kwargs[\"decode_nodes\"] else None\n",
        "\n",
        "        # diag-aggregation for normalization of rows\n",
        "        self.normalize_diag = kwargs[\"normalize_diag\"] if \"normalize_diag\" in kwargs else False\n",
        "        self.diag_aggregate = aggr.SumAggregation()\n",
        "\n",
        "        # normalization\n",
        "        self.graph_norm = pyg.norm.GraphNorm(num_node_features) if (\"graph_norm\" in kwargs and kwargs[\"graph_norm\"]) else None\n",
        "\n",
        "        # drop tolerance and additional fill-ins and more sparsity\n",
        "        self.tau = drop_tol\n",
        "        self.two = kwargs.get(\"two_hop\", False)\n",
        "\n",
        "    def forward(self, data):\n",
        "        # ! data could be batched here...(not implemented)\n",
        "\n",
        "        if self.augment_node_features:\n",
        "            data = augment_features(data, skip_rhs=True)\n",
        "\n",
        "        # add additional edges to the data\n",
        "        if self.two:\n",
        "            data = TwoHop()(data)\n",
        "\n",
        "        # * in principle it is possible to integrate reordering here.\n",
        "\n",
        "        data = ToLowerTriangular()(data)\n",
        "\n",
        "        # get the input data\n",
        "        edge_embedding = data.edge_attr\n",
        "        l_index = data.edge_index\n",
        "\n",
        "        if self.graph_norm is not None:\n",
        "            node_embedding = self.graph_norm(data.x, batch=data.batch)\n",
        "        else:\n",
        "            node_embedding = data.x\n",
        "\n",
        "        # copy the input data (only edges of original matrix A)\n",
        "        a_edges = edge_embedding.clone()\n",
        "\n",
        "        if self.global_features > 0:\n",
        "            global_features = torch.zeros((1, self.global_features), device=data.x.device, requires_grad=False)\n",
        "            # feature ideas: nnz, 1-norm, inf-norm col/row var, min/max variability, avg distances to nnz\n",
        "        else:\n",
        "            global_features = None\n",
        "\n",
        "        # compute the output of the network\n",
        "        for i, layer in enumerate(self.mps):\n",
        "            if i != 0 and self.skip_connections:\n",
        "                edge_embedding = torch.cat([edge_embedding, a_edges], dim=1)\n",
        "\n",
        "            edge_embedding, node_embedding, global_features = layer(node_embedding, l_index, edge_embedding, global_features)\n",
        "\n",
        "        # transform the output into a matrix\n",
        "        return self.transform_output_matrix(node_embedding, l_index, edge_embedding, a_edges)\n",
        "\n",
        "    def transform_output_matrix(self, node_x, edge_index, edge_values, a_edges):\n",
        "        # force diagonal to be positive\n",
        "        diag = edge_index[0] == edge_index[1]\n",
        "\n",
        "        # normalize diag such that it has zero residual\n",
        "        if self.normalize_diag:\n",
        "            # copy the diag of matrix A\n",
        "            a_diag = a_edges[diag]\n",
        "\n",
        "            # compute the row norm\n",
        "            square_values = torch.pow(edge_values, 2)\n",
        "            aggregated = self.diag_aggregate(square_values, edge_index[0])\n",
        "\n",
        "            # now, we renormalize the edge values such that they are the square root of the original value...\n",
        "            edge_values = torch.sqrt(a_diag[edge_index[0]]) * edge_values / torch.sqrt(aggregated[edge_index[0]])\n",
        "\n",
        "        else:\n",
        "            # otherwise, just take the edge values as they are...\n",
        "            # but take the square root as it is numerically better\n",
        "            # edge_values[diag] = torch.exp(edge_values[diag])\n",
        "            edge_values[diag] = torch.sqrt(torch.exp(edge_values[diag]))\n",
        "\n",
        "        # node decoder\n",
        "        node_output = self.node_decoder(node_x).squeeze() if self.node_decoder is not None else None\n",
        "\n",
        "        # ! this if should only be activated when the model is in production!!\n",
        "        if torch.is_inference_mode_enabled():\n",
        "\n",
        "            # we can decide to remove small elements during inference from the preconditioner matrix\n",
        "            if self.tau != 0:\n",
        "                small_value = (torch.abs(edge_values) <= self.tau).squeeze()\n",
        "\n",
        "                # small value and not diagonal\n",
        "                elems = torch.logical_and(small_value, torch.logical_not(diag))\n",
        "\n",
        "                # might be able to do this easily!\n",
        "                edge_values[elems] = 0\n",
        "\n",
        "                # remove zeros from the sparse representation\n",
        "                filt = (edge_values != 0).squeeze()\n",
        "                edge_values = edge_values[filt]\n",
        "                edge_index = edge_index[:, filt]\n",
        "\n",
        "            # ! this is the way to go!!\n",
        "            # Doing pytorch -> scipy -> numml is a lot faster than pytorch -> numml on CPU\n",
        "            # On GPU it is faster to go to pytorch -> numml -> CPU\n",
        "\n",
        "            # convert to scipy sparse matrix\n",
        "            # m = to_scipy_sparse_matrix(edge_index, matrix_values)\n",
        "            m = torch.sparse_coo_tensor(edge_index, edge_values.squeeze(),\n",
        "                                        size=(node_x.size()[0], node_x.size()[0]))\n",
        "                                        # type=torch.double)\n",
        "\n",
        "            # produce L and U seperatly\n",
        "            l = SparseCSRTensor(m)\n",
        "            u = SparseCSRTensor(m.T)\n",
        "\n",
        "            return l, u, node_output\n",
        "\n",
        "        else:\n",
        "            # For training and testing (computing regular losses for examples.)\n",
        "            # does not need to be performance optimized!\n",
        "            # use torch sparse directly\n",
        "            t = torch.sparse_coo_tensor(edge_index, edge_values.squeeze(),\n",
        "                                        size=(node_x.size()[0], node_x.size()[0]))\n",
        "\n",
        "            # normalized l1 norm is best computed here!\n",
        "            # l2_nn = torch.linalg.norm(edge_values, ord=2)\n",
        "            l1_penalty = torch.sum(torch.abs(edge_values)) / len(edge_values)\n",
        "\n",
        "            return t, l1_penalty, node_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "19DkjzdMP-q0"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, width, layer_norm=False, activation=\"relu\", activate_final=False):\n",
        "        super().__init__()\n",
        "        width = list(filter(lambda x: x > 0, width))\n",
        "        assert len(width) >= 2, \"Need at least one layer in the network!\"\n",
        "\n",
        "        lls = nn.ModuleList()\n",
        "        for k in range(len(width)-1):\n",
        "            lls.append(nn.Linear(width[k], width[k+1], bias=True))\n",
        "            if k != (len(width)-2) or activate_final:\n",
        "                if activation == \"relu\":\n",
        "                    lls.append(nn.ReLU())\n",
        "                elif activation == \"tanh\":\n",
        "                    lls.append(nn.Tanh())\n",
        "                elif activation == \"leakyrelu\":\n",
        "                    lls.append(nn.LeakyReLU())\n",
        "                elif activation == \"sigmoid\":\n",
        "                    lls.append(nn.Sigmoid())\n",
        "                else:\n",
        "                    raise NotImplementedError(f\"Activation '{activation}' not implemented\")\n",
        "\n",
        "        if layer_norm:\n",
        "            lls.append(nn.LayerNorm(width[-1]))\n",
        "\n",
        "        self.m = nn.Sequential(*lls)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.m(x)\n",
        "\n",
        "class ToLowerTriangular(torch_geometric.transforms.BaseTransform):\n",
        "    def __init__(self, inplace=False):\n",
        "        self.inplace = inplace\n",
        "\n",
        "    def __call__(self, data, order=None):\n",
        "        if not self.inplace:\n",
        "            data = data.clone()\n",
        "\n",
        "        # TODO: if order is given use that one instead\n",
        "        if order is not None:\n",
        "            raise NotImplementedError(\"Custom ordering not yet implemented...\")\n",
        "\n",
        "        # transform the data into lower triag graph\n",
        "        # this should be a data transformation (maybe?)\n",
        "        rows, cols = data.edge_index[0], data.edge_index[1]\n",
        "        fil = cols <= rows\n",
        "        l_index = data.edge_index[:, fil]\n",
        "        edge_embedding = data.edge_attr[fil]\n",
        "\n",
        "        data.edge_index, data.edge_attr = l_index, edge_embedding\n",
        "        return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jVDRs1QIP5Lw"
      },
      "outputs": [],
      "source": [
        "class MP_Block(nn.Module):\n",
        "    # L@L.T matrix multiplication graph layer\n",
        "    # Aligns the computation of L@L.T - A with the learned updates\n",
        "    def __init__(self, skip_connections, first, last, edge_features, node_features, global_features, hidden_size, **kwargs) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        # first and second aggregation\n",
        "        if \"aggregate\" in kwargs and kwargs[\"aggregate\"] is not None:\n",
        "            aggr = kwargs[\"aggregate\"] if len(kwargs[\"aggregate\"]) == 2 else kwargs[\"aggregate\"] * 2\n",
        "        else:\n",
        "            aggr = [\"mean\", \"sum\"]\n",
        "\n",
        "        act = kwargs[\"activation\"] if \"activation\" in kwargs else \"relu\"\n",
        "\n",
        "        edge_features_in = 1 if first else edge_features\n",
        "        edge_features_out = 1 if last else edge_features\n",
        "\n",
        "        # We use 2 graph nets in order to operate on the upper and lower triangular parts of the matrix\n",
        "        self.l1 = GraphNet(node_features=node_features, edge_features=edge_features_in, global_features=global_features,\n",
        "                           hidden_size=hidden_size, skip_connection=(not first and skip_connections),\n",
        "                           aggregate=aggr[0], activation=act, edge_features_out=edge_features)\n",
        "\n",
        "        self.l2 = GraphNet(node_features=node_features, edge_features=edge_features, global_features=global_features,\n",
        "                           hidden_size=hidden_size, aggregate=aggr[1], activation=act, edge_features_out=edge_features_out)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr, global_features):\n",
        "        edge_embedding, node_embeddings, global_features = self.l1(x, edge_index, edge_attr, g=global_features)\n",
        "\n",
        "        # flip row and column indices\n",
        "        edge_index = torch.stack([edge_index[1], edge_index[0]], dim=0)\n",
        "        edge_embedding, node_embeddings, global_features = self.l2(node_embeddings, edge_index, edge_embedding, g=global_features)\n",
        "\n",
        "        return edge_embedding, node_embeddings, global_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "tLw1q2nYR6hl"
      },
      "outputs": [],
      "source": [
        "def get_dataloader(dataset, n=0, batch_size=1, spd=True, mode=\"train\", size=None, graph=True):\n",
        "    # Setup datasets\n",
        "\n",
        "    if dataset == \"random\":\n",
        "        data = FolderDataset(f\"./dataset/{mode}/\", n, size=size, graph=graph)\n",
        "\n",
        "    else:\n",
        "        raise NotImplementedError(\"Dataset not implemented, Available: random\")\n",
        "\n",
        "    # Data Loaders\n",
        "    if mode == \"train\":\n",
        "        dataloader = DataLoader(data, batch_size=batch_size, shuffle=True)\n",
        "    else:\n",
        "        dataloader = DataLoader(data, batch_size=1, shuffle=False)\n",
        "\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "m8StoNdBTKu4"
      },
      "outputs": [],
      "source": [
        "class Preconditioner:\n",
        "    def __init__(self, A, **kwargs):\n",
        "        self.breakdown = False\n",
        "        self.nnz = 0\n",
        "        self.time = 0\n",
        "        self.n = kwargs.get(\"n\", 0)\n",
        "\n",
        "    def timed_setup(self, A, **kwargs):\n",
        "        start = time_function()\n",
        "        self.setup(A, **kwargs)\n",
        "        stop = time_function()\n",
        "        self.time = stop - start\n",
        "\n",
        "    def get_inverse(self):\n",
        "        ones = torch.ones(self.n)\n",
        "        offset = torch.zeros(1).to(torch.int64)\n",
        "\n",
        "        I = torch.sparse.spdiags(ones, offset, (self.n, self.n))\n",
        "        I = I.to(torch.float64)\n",
        "\n",
        "        return I\n",
        "\n",
        "    def get_p_matrix(self):\n",
        "        return self.get_inverse()\n",
        "\n",
        "    def check_breakdown(self, P):\n",
        "        if np.isnan(np.min(P)):\n",
        "            self.breakdown = True\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return x\n",
        "\n",
        "class LearnedPreconditioner(Preconditioner):\n",
        "    def __init__(self, data, model, **kwargs):\n",
        "        super().__init__(data, **kwargs)\n",
        "\n",
        "        self.model = model\n",
        "        self.spd = isinstance(model, NeuralIF)\n",
        "\n",
        "        self.timed_setup(data, **kwargs)\n",
        "\n",
        "        if self.spd:\n",
        "            self.nnz = self.L.nnz\n",
        "        else:\n",
        "            self.nnz = self.L.nnz + self.U.nnz - data.x.shape[0]\n",
        "\n",
        "    def setup(self, data, **kwargs):\n",
        "        L, U, _ = self.model(data)\n",
        "\n",
        "        self.L = L.to(\"cpu\").to(torch.float64)\n",
        "        self.U = U.to(\"cpu\").to(torch.float64)\n",
        "\n",
        "    def get_inverse(self):\n",
        "        L_inv = torch.inverse(self.L.to_dense())\n",
        "        U_inv = torch.inverse(self.U.to_dense())\n",
        "\n",
        "        return U_inv@L_inv\n",
        "\n",
        "    def get_p_matrix(self):\n",
        "        return self.L@self.U\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return fb_solve(self.L, self.U, x, unit_upper=not self.spd)\n",
        "\n",
        "\n",
        "def fb_solve(L, U, r, unit_lower=False, unit_upper=False):\n",
        "    y = L.solve_triangular(upper=False, unit=unit_lower, b=r)\n",
        "    z = U.solve_triangular(upper=True, unit=unit_upper, b=y)\n",
        "    return z\n",
        "\n",
        "\n",
        "def fb_solve_joint(LU, r):\n",
        "    # Note: solve triangular ignores the values in lower/upper triangle\n",
        "    y = LU.solve_triangular(upper=False, unit=False, b=r)\n",
        "    z = LU.solve_triangular(upper=True, unit=False, b=y)\n",
        "    return z\n",
        "\n",
        "time_function = lambda: time.perf_counter()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsvyF-64N506"
      },
      "source": [
        "# **VALIDATION FUNCTION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "lmG0zvFyN3vj"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def validate(model, validation_loader, solve=False, solver=\"cg\", **kwargs):\n",
        "    model.eval()\n",
        "\n",
        "    acc_loss = 0.0\n",
        "    num_loss = 0\n",
        "    acc_solver_iters = 0.0\n",
        "\n",
        "    for i, data in enumerate(validation_loader):\n",
        "        data = data.to(device)\n",
        "\n",
        "        # construct problem data\n",
        "        A, b = graph_to_matrix(data)\n",
        "\n",
        "        # run conjugate gradient method\n",
        "        # this requires the learned preconditioner to be reasonably good!\n",
        "        if solve:\n",
        "            # run CG on CPU\n",
        "            with torch.inference_mode():\n",
        "                preconditioner = LearnedPreconditioner(data, model)\n",
        "\n",
        "            A = A.to(\"cpu\").to(torch.float64)\n",
        "            b = b.to(\"cpu\").to(torch.float64)\n",
        "            x_init = None\n",
        "\n",
        "            solver_start = time.time()\n",
        "\n",
        "            if solver == \"cg\":\n",
        "                l, x_hat = preconditioned_conjugate_gradient(A.to(\"cpu\"), b.to(\"cpu\"), M=preconditioner,\n",
        "                                                             x0=x_init, rtol=1e-6, max_iter=1_000)\n",
        "            elif solver == \"gmres\":\n",
        "                l, x_hat = gmres(A, b, M=preconditioner, x0=x_init, atol=1e-6, max_iter=1_000, left=False)\n",
        "            else:\n",
        "                raise NotImplementedError(\"Solver not implemented choose between CG and GMRES!\")\n",
        "\n",
        "            solver_stop = time.time()\n",
        "\n",
        "            # Measure preconditioning performance\n",
        "            solver_time = (solver_stop - solver_start)\n",
        "            acc_solver_iters += len(l) - 1\n",
        "\n",
        "        else:\n",
        "            output, _, _ = model(data)\n",
        "\n",
        "            # Here, we compute the loss using the full forbenius norm (no estimator)\n",
        "            # l = frobenius_loss(output, A)\n",
        "\n",
        "            l = loss(data, output, config=None) ##USe the default loss instead of the frobenius loss\n",
        "\n",
        "            acc_loss += l.item()\n",
        "            num_loss += 1\n",
        "\n",
        "    if solve:\n",
        "        # print(f\"Smallest eigenvalue: {dist[0]}\")\n",
        "        print(f\"Validation\\t iterations:\\t{acc_solver_iters / len(validation_loader):.2f}\")\n",
        "        return acc_solver_iters / len(validation_loader)\n",
        "\n",
        "    else:\n",
        "        print(f\"Validation loss:\\t{acc_loss / num_loss:.2f}\")\n",
        "        return acc_loss / len(validation_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsVE6R9SOLkT"
      },
      "source": [
        "# MAIN CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "E4ci3LbMOPEG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number params in model: 460\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "999it [08:05, 21.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation\t iterations:\t287.80\n",
            "287.8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1000it [08:05,  2.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 \t loss: 517.5744089355469 \t time: 485.9105565000209\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "999it [08:14, 20.78s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation\t iterations:\t270.70\n",
            "270.7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1000it [08:15,  2.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 \t loss: 367.9119475708008 \t time: 495.18716924998444\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "999it [08:17, 20.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation\t iterations:\t267.60\n",
            "267.6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1000it [08:17,  2.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 \t loss: 357.13817221069337 \t time: 497.9681067909987\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "999it [09:17, 21.45s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation\t iterations:\t267.80\n",
            "267.8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1000it [09:18,  1.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 \t loss: 353.96421224975586 \t time: 556.3167774169997\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "999it [08:02, 20.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation\t iterations:\t268.50\n",
            "268.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1000it [08:02,  2.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 \t loss: 350.8254994812012 \t time: 482.6419573329913\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "999it [08:00, 20.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation\t iterations:\t269.20\n",
            "269.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1000it [08:00,  2.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 \t loss: 348.95432199096683 \t time: 480.44848229098716\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "999it [07:58, 20.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation\t iterations:\t269.90\n",
            "269.9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1000it [07:58,  2.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 \t loss: 347.16505059814455 \t time: 478.91117370899883\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "999it [08:00, 20.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation\t iterations:\t269.50\n",
            "269.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1000it [08:00,  2.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8 \t loss: 345.94179406738283 \t time: 480.5256363330118\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "999it [08:00, 20.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation\t iterations:\t270.60\n",
            "270.6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1000it [08:00,  2.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9 \t loss: 344.573157409668 \t time: 480.91597533301683\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "999it [07:57, 20.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation\t iterations:\t270.80\n",
            "270.8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1000it [07:57,  2.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10 \t loss: 343.7699348144531 \t time: 477.79206800000975\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "999it [07:56, 20.31s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation\t iterations:\t271.40\n",
            "271.4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1000it [07:56,  2.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11 \t loss: 342.9541725158692 \t time: 476.48587433301145\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "999it [07:56, 20.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation\t iterations:\t270.60\n",
            "270.6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1000it [07:57,  2.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12 \t loss: 342.3806929016113 \t time: 477.0009245830006\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "999it [07:59, 20.39s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation\t iterations:\t271.70\n",
            "271.7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1000it [07:59,  2.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13 \t loss: 341.4950877990723 \t time: 479.82606774999294\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "698it [05:06,  2.28it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 85\u001b[39m\n\u001b[32m     80\u001b[39m l = loss(output, data, c=reg, config=config[\u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     82\u001b[39m \u001b[38;5;66;03m#  if reg:\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[38;5;66;03m#    l = l + config[\"regularizer\"] * reg\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m \u001b[43ml\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m running_loss += l.item()\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# track the gradient norm\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/graph/lib/python3.12/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/graph/lib/python3.12/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/graph/lib/python3.12/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "from tqdm import tqdm\n",
        "if config[\"save\"]:\n",
        "        os.makedirs(folder, exist_ok=True)\n",
        "        save_dict_to_file(config, os.path.join(folder, \"config.json\"))\n",
        "\n",
        "# global seed-ish\n",
        "torch_geometric.seed_everything(config[\"seed\"])\n",
        "\n",
        "# args for the model\n",
        "model_args = {k: config[k] for k in [\"latent_size\", \"message_passing_steps\", \"skip_connections\",\n",
        "                                      \"augment_nodes\", \"global_features\", \"decode_nodes\",\n",
        "                                      \"normalize_diag\", \"activation\", \"aggregate\", \"graph_norm\",\n",
        "                                      \"two_hop\", \"edge_features\", \"normalize\"]\n",
        "              if k in config}\n",
        "\n",
        "# run the GMRES algorithm instead of CG (?)\n",
        "gmres = False\n",
        "\n",
        "# Create model\n",
        "if config[\"model\"] == \"neuralpcg\":\n",
        "    model = NeuralPCG(**model_args)\n",
        "\n",
        "elif config[\"model\"] == \"nif\" or config[\"model\"] == \"neuralif\" or config[\"model\"] == \"inf\":\n",
        "    model = NeuralIF(**model_args)\n",
        "\n",
        "elif config[\"model\"] == \"precondnet\":\n",
        "    model = PreCondNet(**model_args)\n",
        "\n",
        "elif config[\"model\"] == \"lu\" or config[\"model\"] == \"learnedlu\":\n",
        "    gmres = True\n",
        "    model = LearnedLU(**model_args)\n",
        "\n",
        "else:\n",
        "    raise NotImplementedError\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "print(f\"Number params in model: {count_parameters(model)}\")\n",
        "print()\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters())\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=20)\n",
        "\n",
        "# Setup datasets\n",
        "train_loader = get_dataloader(config[\"dataset\"], config[\"n\"], config[\"batch_size\"],\n",
        "                              spd=not gmres, mode=\"train\")\n",
        "\n",
        "validation_loader = get_dataloader(config[\"dataset\"], config[\"n\"], 1, spd=(not gmres), mode=\"val\")\n",
        "\n",
        "best_val = float(\"inf\")\n",
        "logger = TrainResults(folder)\n",
        "\n",
        "# todo: compile the model\n",
        "# compiled_model = torch.compile(model, mode=\"reduce-overhead\")\n",
        "# model = torch_geometric.compile(model, mode=\"reduce-overhead\")\n",
        "\n",
        "total_it = 0\n",
        "\n",
        "# Train loop\n",
        "for epoch in range(config[\"num_epochs\"]):\n",
        "    running_loss = 0.0\n",
        "    grad_norm = 0.0\n",
        "\n",
        "    start_epoch = time.perf_counter()\n",
        "\n",
        "    for it, data in tqdm(enumerate(train_loader)):\n",
        "        # increase iteration count\n",
        "        total_it += 1\n",
        "\n",
        "        # enable training mode\n",
        "        model.train()\n",
        "\n",
        "        # print(data)\n",
        "        # print(data.x)\n",
        "        start = time.perf_counter()\n",
        "        data = data.to(device)\n",
        "\n",
        "        output, reg, _ = model(data)\n",
        "        l = loss(output, data, c=reg, config=config[\"loss\"])\n",
        "\n",
        "        #  if reg:\n",
        "        #    l = l + config[\"regularizer\"] * reg\n",
        "\n",
        "        l.backward()\n",
        "        running_loss += l.item()\n",
        "\n",
        "        # track the gradient norm\n",
        "        if \"gradient_clipping\" in config and config[\"gradient_clipping\"]:\n",
        "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config[\"gradient_clipping\"])\n",
        "\n",
        "        else:\n",
        "            total_norm = 0.0\n",
        "\n",
        "            for p in model.parameters():\n",
        "                if p.grad is not None:\n",
        "                    param_norm = p.grad.detach().data.norm(2)\n",
        "                    total_norm += param_norm.item() ** 2\n",
        "\n",
        "            grad_norm = total_norm ** 0.5 / config[\"batch_size\"]\n",
        "\n",
        "        # update network parameters\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logger.log(l.item(), grad_norm, time.perf_counter() - start)\n",
        "\n",
        "        # Do validation after 100 updates (to support big datasets)\n",
        "        # convergence is expected to be pretty fast...\n",
        "        if (total_it + 1) % 1000 == 0:\n",
        "\n",
        "            # start with cg-checks after 5 iterations\n",
        "            val_its = validate(model, validation_loader, solve=True,\n",
        "                                solver=\"gmres\" if gmres else \"cg\")\n",
        "\n",
        "            # use scheduler\n",
        "            # if config[\"scheduler\"]:\n",
        "            #    scheduler.step(val_loss)\n",
        "\n",
        "            logger.log_val(None, val_its)\n",
        "\n",
        "            # val_perf = val_cgits if val_cgits > 0 else val_loss\n",
        "            val_perf = val_its\n",
        "            print(val_perf)\n",
        "\n",
        "            if val_perf < best_val:\n",
        "                if config[\"save\"]:\n",
        "                    torch.save(model.state_dict(), f\"{folder}/best_model.pt\")\n",
        "                best_val = val_perf\n",
        "\n",
        "    epoch_time = time.perf_counter() - start_epoch\n",
        "\n",
        "    # save model every epoch for analysis...\n",
        "    if config[\"save\"]:\n",
        "        torch.save(model.state_dict(), f\"{folder}/model_epoch{epoch+1}.pt\")\n",
        "\n",
        "    print(f\"Epoch {epoch+1} \\t loss: {1/len(train_loader) * running_loss} \\t time: {epoch_time}\")\n",
        "\n",
        "# save fully trained model\n",
        "if config[\"save\"]:\n",
        "    logger.save_results()\n",
        "    torch.save(model.to(torch.float).state_dict(), f\"{folder}/final_model.pt\")\n",
        "\n",
        "# Test the model\n",
        "# wandb.run.summary[\"validation_chol\"] = best_val\n",
        "print()\n",
        "print(\"Best validation loss:\", best_val)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnSu_43OXbsw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number params in model: 460\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:06,  6.49s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 \t loss: 648.0010375976562 \t time: 6.498716834001243\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:06,  6.17s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 \t loss: 644.8334350585938 \t time: 6.175045791984303\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:06,  6.26s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 \t loss: 644.45849609375 \t time: 6.265549583011307\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:06,  6.28s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 \t loss: 648.1729736328125 \t time: 6.28558462500223\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:06,  6.28s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 \t loss: 661.9956665039062 \t time: 6.277478999982122\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:06,  6.35s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 \t loss: 650.3807983398438 \t time: 6.3484238749952056\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:00, 14.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 \t loss: 629.5044555664062 \t time: 0.06798545800847933\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:06,  6.31s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8 \t loss: 672.1594848632812 \t time: 6.30877825000789\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:06,  6.30s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9 \t loss: 633.2813720703125 \t time: 6.3031272499938495\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:15, 15.31s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation\t iterations:\t1000.00\n",
            "Epoch 10 \t loss: 658.324951171875 \t time: 15.311978582991287\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:06,  6.24s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11 \t loss: 630.0914916992188 \t time: 6.245356749976054\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:00, 14.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12 \t loss: 627.0044555664062 \t time: 0.06777891700039618\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:06,  6.31s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13 \t loss: 649.7213134765625 \t time: 6.307013624988031\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:00, 14.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14 \t loss: 648.6812744140625 \t time: 0.06939237500773743\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:06,  6.24s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15 \t loss: 682.5653076171875 \t time: 6.235936249984661\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:06,  6.16s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16 \t loss: 636.4898681640625 \t time: 6.157793249993119\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:08,  8.56s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17 \t loss: 661.7369384765625 \t time: 8.560618791001616\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:00,  5.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18 \t loss: 644.547119140625 \t time: 0.17553054197924212\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:06,  6.34s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19 \t loss: 620.40380859375 \t time: 6.345016290986678\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:15, 15.11s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation\t iterations:\t1000.00\n",
            "Epoch 20 \t loss: 660.9364013671875 \t time: 15.111671457998455\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:00, 15.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21 \t loss: 635.2854614257812 \t time: 0.06703549998928793\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:06,  6.34s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22 \t loss: 644.2913208007812 \t time: 6.340084083989495\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:08,  8.17s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23 \t loss: 628.8077392578125 \t time: 8.169379583006958\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:00, 15.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24 \t loss: 579.22509765625 \t time: 0.06687212499673478\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:09,  9.15s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25 \t loss: 649.2479858398438 \t time: 9.14973970799474\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:06,  6.49s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 26 \t loss: 597.6178588867188 \t time: 6.487424667022424\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:06,  6.15s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 27 \t loss: 632.5523681640625 \t time: 6.148694792005699\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:00, 14.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 28 \t loss: 620.0474243164062 \t time: 0.07086966600036249\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:06,  6.18s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 29 \t loss: 616.0174560546875 \t time: 6.179065167001681\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:15, 15.32s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation\t iterations:\t1000.00\n",
            "Epoch 30 \t loss: 634.068603515625 \t time: 15.319460375001654\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:06,  6.24s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 31 \t loss: 615.252197265625 \t time: 6.237149166001473\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:06,  6.16s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 32 \t loss: 586.6155395507812 \t time: 6.165363124979194\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:06,  6.21s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 33 \t loss: 607.7601318359375 \t time: 6.211012707994087\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:00, 14.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 34 \t loss: 585.810302734375 \t time: 0.06811220801318996\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:06,  6.15s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 35 \t loss: 590.9260864257812 \t time: 6.154999124992173\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:00, 14.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 36 \t loss: 600.8659057617188 \t time: 0.06935170799260959\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:06,  6.22s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 37 \t loss: 568.624267578125 \t time: 6.219065666984534\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:00, 14.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 38 \t loss: 616.122802734375 \t time: 0.07188670799951069\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:00, 14.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 39 \t loss: 572.6077880859375 \t time: 0.07194279201212339\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:09,  9.35s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation\t iterations:\t1000.00\n",
            "Epoch 40 \t loss: 600.86865234375 \t time: 9.351634042017395\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:06,  6.23s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 41 \t loss: 612.6912231445312 \t time: 6.233692207984859\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:00, 15.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 42 \t loss: 559.1852416992188 \t time: 0.06566987498081289\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:06,  6.23s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 43 \t loss: 608.8047485351562 \t time: 6.233021374995587\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:00, 13.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 44 \t loss: 635.92626953125 \t time: 0.07356741701369174\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:00, 14.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 45 \t loss: 586.8803100585938 \t time: 0.07026966701960191\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:06,  6.26s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 46 \t loss: 593.5850830078125 \t time: 6.261091791006038\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:06,  6.28s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 47 \t loss: 681.6267700195312 \t time: 6.276212958007818\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:06,  6.24s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 48 \t loss: 566.6027221679688 \t time: 6.24294545900193\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:06,  6.38s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 49 \t loss: 598.2230224609375 \t time: 6.381671875016764\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:09,  9.24s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation\t iterations:\t1000.00\n",
            "Epoch 50 \t loss: 612.3037719726562 \t time: 9.241110875009326\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:00, 13.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 51 \t loss: 616.7483520507812 \t time: 0.07662987502408214\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:06,  6.26s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 52 \t loss: 587.8331298828125 \t time: 6.261465791991213\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:00, 13.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 53 \t loss: 611.2992553710938 \t time: 0.07574324999586679\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:06,  6.26s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 54 \t loss: 558.134765625 \t time: 6.260391041985713\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:00, 14.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 55 \t loss: 585.4784545898438 \t time: 0.07014537500799634\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:06,  6.22s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 56 \t loss: 597.5950927734375 \t time: 6.216192541993223\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:00, 14.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 57 \t loss: 575.9331665039062 \t time: 0.07063495900365524\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:06,  6.26s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 58 \t loss: 608.222900390625 \t time: 6.261052207992179\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:06,  6.27s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 59 \t loss: 560.3479614257812 \t time: 6.2714502080052625\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:09,  9.27s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation\t iterations:\t1000.00\n",
            "Epoch 60 \t loss: 629.9467163085938 \t time: 9.273060791019816\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:06,  6.24s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 61 \t loss: 566.429443359375 \t time: 6.243187458021566\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:06,  6.27s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 62 \t loss: 579.3529052734375 \t time: 6.271157542010769\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:00, 13.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 63 \t loss: 587.9664916992188 \t time: 0.07619120800518431\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:00, 14.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 64 \t loss: 547.9495849609375 \t time: 0.06788666700595059\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:06,  6.26s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 65 \t loss: 542.1576538085938 \t time: 6.2580850000085775\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:06,  6.25s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 66 \t loss: 563.93505859375 \t time: 6.253113667014986\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:00,  9.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 67 \t loss: 545.6659545898438 \t time: 0.10279995799646713\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:06,  6.24s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 68 \t loss: 570.7387084960938 \t time: 6.237256124994019\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:00, 13.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 69 \t loss: 567.3051147460938 \t time: 0.07519554102327675\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:15, 15.38s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation\t iterations:\t1000.00\n",
            "Epoch 70 \t loss: 517.20166015625 \t time: 15.382228417001897\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:06,  6.22s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 71 \t loss: 557.7051391601562 \t time: 6.219496709003579\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:06,  6.24s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 72 \t loss: 566.9926147460938 \t time: 6.2441775419865735\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:06,  6.29s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 73 \t loss: 522.4611206054688 \t time: 6.291806624998571\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:00, 10.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 74 \t loss: 521.5474853515625 \t time: 0.09941475000232458\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:00, 13.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 75 \t loss: 523.2269897460938 \t time: 0.07529162499122322\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:00, 13.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 76 \t loss: 543.7357177734375 \t time: 0.07482095900923014\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:06,  6.37s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 77 \t loss: 524.1746215820312 \t time: 6.369766249990789\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:00, 14.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 78 \t loss: 560.7022094726562 \t time: 0.0711592499865219\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:06,  6.37s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 79 \t loss: 534.071044921875 \t time: 6.366313332982827\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:08,  8.39s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation\t iterations:\t656.00\n",
            "Epoch 80 \t loss: 542.5164184570312 \t time: 8.390458750014659\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:00, 13.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 81 \t loss: 531.866943359375 \t time: 0.07721358400885947\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:06,  6.32s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 82 \t loss: 485.89306640625 \t time: 6.322980500000995\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:06,  6.44s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 83 \t loss: 504.1593017578125 \t time: 6.435696417000145\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:00, 11.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 84 \t loss: 512.3087158203125 \t time: 0.08645895801601\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:00, 14.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 85 \t loss: 503.5458984375 \t time: 0.06776350000291131\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:00, 12.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 86 \t loss: 450.6434631347656 \t time: 0.07888179100700654\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:00, 15.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 87 \t loss: 463.2708435058594 \t time: 0.06582466600229964\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:00, 15.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 88 \t loss: 478.26776123046875 \t time: 0.06573383300565183\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:06,  6.31s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 89 \t loss: 429.6103210449219 \t time: 6.31205070798751\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:06,  6.57s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation\t iterations:\t263.00\n",
            "Epoch 90 \t loss: 455.484375 \t time: 6.569106167007703\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:00, 15.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 91 \t loss: 442.7342529296875 \t time: 0.06727133301319554\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:05,  5.64s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 92 \t loss: 426.3216857910156 \t time: 5.637004000018351\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:05,  5.69s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 93 \t loss: 436.168701171875 \t time: 5.689788499992574\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:05,  5.72s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 94 \t loss: 416.42041015625 \t time: 5.718342833017232\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:00, 16.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 95 \t loss: 393.9168395996094 \t time: 0.06227674998808652\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:05,  5.65s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 96 \t loss: 422.5009765625 \t time: 5.648008999996819\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:06,  6.30s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 97 \t loss: 396.6545104980469 \t time: 6.300569084007293\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:05,  5.72s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 98 \t loss: 381.119873046875 \t time: 5.718054124998162\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:05,  5.87s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 99 \t loss: 376.19427490234375 \t time: 5.8698034579865634\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]"
          ]
        }
      ],
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "from tqdm import tqdm\n",
        "if config[\"save\"]:\n",
        "        os.makedirs(folder, exist_ok=True)\n",
        "        save_dict_to_file(config, os.path.join(folder, \"config.json\"))\n",
        "\n",
        "# global seed-ish\n",
        "torch_geometric.seed_everything(config[\"seed\"])\n",
        "\n",
        "# args for the model\n",
        "model_args = {k: config[k] for k in [\"latent_size\", \"message_passing_steps\", \"skip_connections\",\n",
        "                                      \"augment_nodes\", \"global_features\", \"decode_nodes\",\n",
        "                                      \"normalize_diag\", \"activation\", \"aggregate\", \"graph_norm\",\n",
        "                                      \"two_hop\", \"edge_features\", \"normalize\"]\n",
        "              if k in config}\n",
        "\n",
        "# run the GMRES algorithm instead of CG (?)\n",
        "gmres = False\n",
        "\n",
        "# Create model\n",
        "if config[\"model\"] == \"neuralpcg\":\n",
        "    model = NeuralPCG(**model_args)\n",
        "\n",
        "elif config[\"model\"] == \"nif\" or config[\"model\"] == \"neuralif\" or config[\"model\"] == \"inf\":\n",
        "    model = NeuralIF(**model_args)\n",
        "\n",
        "elif config[\"model\"] == \"precondnet\":\n",
        "    model = PreCondNet(**model_args)\n",
        "\n",
        "elif config[\"model\"] == \"lu\" or config[\"model\"] == \"learnedlu\":\n",
        "    gmres = True\n",
        "    model = LearnedLU(**model_args)\n",
        "\n",
        "else:\n",
        "    raise NotImplementedError\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "print(f\"Number params in model: {count_parameters(model)}\")\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters())\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=20)\n",
        "\n",
        "# Setup datasets with ClusterGCN\n",
        "train_loader = get_cluster_dataloader(\n",
        "    config[\"dataset\"],\n",
        "    config[\"n\"],\n",
        "    config[\"batch_size\"],\n",
        "    spd=True,\n",
        "    mode=\"train\",\n",
        "    num_clusters=config[\"num_clusters\"],\n",
        "    clusters_per_batch=config[\"clusters_per_batch\"]\n",
        ")\n",
        "\n",
        "validation_loader = get_cluster_dataloader(\n",
        "    config[\"dataset\"],\n",
        "    config[\"n\"],\n",
        "    1,\n",
        "    spd=True,\n",
        "    mode=\"val\",\n",
        "    num_clusters=config[\"num_clusters\"],\n",
        "    clusters_per_batch=config[\"clusters_per_batch\"]\n",
        ")\n",
        "\n",
        "best_val = float(\"inf\")\n",
        "logger = TrainResults(folder)\n",
        "total_it = 0\n",
        "\n",
        "# Train loop (similar to your existing code)\n",
        "for epoch in range(config[\"num_epochs\"]):\n",
        "    running_loss = 0.0\n",
        "    grad_norm = 0.0\n",
        "    start_epoch = time.perf_counter()\n",
        "\n",
        "    for it, data in tqdm(enumerate(train_loader)):\n",
        "        total_it += 1\n",
        "        model.train()\n",
        "\n",
        "        start = time.perf_counter()\n",
        "        data = data.to(device)\n",
        "\n",
        "        # Your existing training logic\n",
        "        output, reg, _ = model(data)\n",
        "        l = loss(output, data, c=reg, config=config[\"loss\"])\n",
        "\n",
        "        l.backward()\n",
        "        running_loss += l.item()\n",
        "\n",
        "        # Gradient handling (your existing code)\n",
        "        if \"gradient_clipping\" in config and config[\"gradient_clipping\"]:\n",
        "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config[\"gradient_clipping\"])\n",
        "        else:\n",
        "            total_norm = 0.0\n",
        "            for p in model.parameters():\n",
        "                if p.grad is not None:\n",
        "                    param_norm = p.grad.detach().data.norm(2)\n",
        "                    total_norm += param_norm.item() ** 2\n",
        "            grad_norm = total_norm ** 0.5 / config[\"batch_size\"]\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logger.log(l.item(), grad_norm, time.perf_counter() - start)\n",
        "\n",
        "        # Validation (your existing logic)\n",
        "        if (total_it) % 10 == 0:\n",
        "            val_its = validate(model, validation_loader, solve=True, solver=\"cg\")\n",
        "            logger.log_val(None, val_its)\n",
        "\n",
        "            if val_its < best_val:\n",
        "                if config[\"save\"]:\n",
        "                    torch.save(model.state_dict(), f\"{folder}/best_model.pt\")\n",
        "                best_val = val_its\n",
        "\n",
        "    epoch_time = time.perf_counter() - start_epoch\n",
        "\n",
        "    if config[\"save\"]:\n",
        "        torch.save(model.state_dict(), f\"{folder}/model_epoch{epoch+1}.pt\")\n",
        "\n",
        "    print(f\"Epoch {epoch+1} \\t loss: {1/len(train_loader) * running_loss} \\t time: {epoch_time}\")\n",
        "\n",
        "# Final save\n",
        "if config[\"save\"]:\n",
        "    logger.save_results()\n",
        "    torch.save(model.to(torch.float).state_dict(), f\"{folder}/final_model.pt\")\n",
        "\n",
        "print(\"Best validation performance:\", best_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9EwW-6nIfrC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "graph",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
