{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzFXzJMYMK7H"
      },
      "source": [
        "# **ENVIRONMENT SETUP**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "P3h_0tmhMAK3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -q pyg-lib -f https://data.pyg.org/whl/torch-2.6.0+cu124.html\n",
        "%pip install -q torch torchvision torchaudio\n",
        "%pip install -q torch-scatter -f https://data.pyg.org/whl/torch-2.6.0+cu124.html  # prevents the wheel from taking forever to build\n",
        "%pip install -q torch-sparse -f https://data.pyg.org/whl/torch-2.6.0+cu124.html   # prevents the wheel from taking forever to build\n",
        "%pip install -q torch-cluster -f https://data.pyg.org/whl/torch-2.6.0+cu124.html  # prevents the wheel from taking forever to build\n",
        "%pip install -q torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "wCEW-VWFL72r"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.7.1+cu126\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/home/nicholas_tyy/Documents/TU_Delft/CS4350/proj/GML-Project'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ToKfNRvRkxGc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: '../Documents/Y3S2/CS4350/GML-Project'\n",
            "/home/nicholas_tyy/Documents/TU_Delft/CS4350/proj/GML-Project\n"
          ]
        }
      ],
      "source": [
        "#Change based on project structure\n",
        "%cd ../Documents/Y3S2/CS4350/GML-Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "lqhvjS5eMJnf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import datetime\n",
        "import time\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch_geometric\n",
        "import torch.nn as nn\n",
        "import torch_geometric.nn as pyg\n",
        "from torch_geometric.nn import aggr\n",
        "from torch_geometric.data import Batch, Data\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.utils import subgraph\n",
        "from tqdm import tqdm\n",
        "\n",
        "from apps.data import graph_to_matrix, FolderDataset\n",
        "from neuralif.utils import (\n",
        "    count_parameters, save_dict_to_file,\n",
        "    TwoHop\n",
        ")\n",
        "from neuralif.logger import TrainResults\n",
        "from neuralif.loss import loss\n",
        "\n",
        "from krylov.cg import preconditioned_conjugate_gradient\n",
        "from krylov.gmres import gmres\n",
        "\n",
        "from numml.sparse import SparseCSRTensor\n",
        "\n",
        "import metis  # pip install metis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ecrpeZYaQSbH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(f\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zwz-aA6XQt5j"
      },
      "source": [
        "# **DATASET CREATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "aBjOJC57Qxhc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating 100 samples for the train dataset.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 2/100 [00:07<05:49,  3.57s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m alpha=\u001b[32m10e-4\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m#create_dataset(size, number of samples, density, mode, seed, is_graph, compute_solution)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m create_dataset(n, \u001b[32m100\u001b[39m, alpha=alpha, mode=\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m, rs=\u001b[32m0\u001b[39m, graph=\u001b[38;5;28;01mTrue\u001b[39;00m, solution=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      7\u001b[39m create_dataset(n, \u001b[32m10\u001b[39m, alpha=alpha, mode=\u001b[33m'\u001b[39m\u001b[33mval\u001b[39m\u001b[33m'\u001b[39m, rs=\u001b[32m10000\u001b[39m, graph=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      8\u001b[39m create_dataset(n, \u001b[32m10\u001b[39m, alpha=alpha, mode=\u001b[33m'\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m'\u001b[39m, rs=\u001b[32m103600\u001b[39m, graph=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/TU_Delft/CS4350/proj/GML-Project/apps/synthetic.py:92\u001b[39m, in \u001b[36mcreate_dataset\u001b[39m\u001b[34m(n, samples, alpha, sparsity, graph, rs, mode, solution)\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGenerating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msamples\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m samples for the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m dataset.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m sam \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(samples)):\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     A, x, b = generate_sparse_random(n, random_state=(rs + sam), alpha=alpha, sparsity=sparsity, sol=solution,\n\u001b[32m     93\u001b[39m                                      ood=(mode == \u001b[33m\"\u001b[39m\u001b[33mtest_ood\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m graph:\n\u001b[32m     95\u001b[39m         graph = matrix_to_graph(A, b)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/TU_Delft/CS4350/proj/GML-Project/apps/synthetic.py:66\u001b[39m, in \u001b[36mgenerate_sparse_random\u001b[39m\u001b[34m(n, alpha, sparsity, random_state, sol, ood)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# compute the solution\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sol:\n\u001b[32m     65\u001b[39m     \u001b[38;5;66;03m# generate solution using dense method for accuracy reasons\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     x, _ = spla.cg(A, b)\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     69\u001b[39m     x = \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/GML_proj/lib/python3.12/site-packages/scipy/sparse/linalg/_isolve/iterative.py:417\u001b[39m, in \u001b[36mcg\u001b[39m\u001b[34m(A, b, x0, rtol, atol, maxiter, M, callback)\u001b[39m\n\u001b[32m    414\u001b[39m     r -= alpha*q\n\u001b[32m    415\u001b[39m     rho_prev = rho_cur\n\u001b[32m--> \u001b[39m\u001b[32m417\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[32m    418\u001b[39m         callback(x)\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# for loop exhausted\u001b[39;00m\n\u001b[32m    421\u001b[39m     \u001b[38;5;66;03m# Return incomplete progress\u001b[39;00m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "from apps.synthetic import create_dataset\n",
        "n = 10_000\n",
        "alpha=10e-4\n",
        "\n",
        "#create_dataset(size, number of samples, density, mode, seed, is_graph, compute_solution)\n",
        "create_dataset(n, 100, alpha=alpha, mode='train', rs=0, graph=True, solution=True)\n",
        "create_dataset(n, 10, alpha=alpha, mode='val', rs=10000, graph=True)\n",
        "create_dataset(n, 10, alpha=alpha, mode='test', rs=103600, graph=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **CONFIG**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bMM1n0TOVNW"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"name\": \"experiment_6\",\n",
        "    \"save\": True,\n",
        "    \"seed\": 42,\n",
        "    \"n\": 10000,\n",
        "    \"batch_size\": 1,\n",
        "    \"num_epochs\": 100,\n",
        "    \"dataset\": \"random\",\n",
        "    \"loss\": None,\n",
        "    \"gradient_clipping\": 1.0,\n",
        "    \"regularizer\": 0.0,\n",
        "    \"scheduler\": False,\n",
        "    \"model\": \"neuralif\",\n",
        "    \"normalize\": False,\n",
        "    \"latent_size\": 8,\n",
        "    \"message_passing_steps\": 3,\n",
        "    \"decode_nodes\": False,\n",
        "    \"normalize_diag\": False,\n",
        "    \"aggregate\": [\"mean\", \"sum\"],\n",
        "    \"activation\": \"relu\",\n",
        "    \"skip_connections\": True,\n",
        "    \"augment_nodes\": False,\n",
        "    \"global_features\": 0,\n",
        "    \"edge_features\": 1,\n",
        "    \"graph_norm\": False,\n",
        "    \"two_hop\": False,\n",
        "    \"samples\": 10,\n",
        "    \"num_neighbors\": [15, 10],  # number of neighbours to sample in each hop (GraphSAGE sampling)\n",
        "    \"num_clusters\": 10,  # Number of clusters to partition each graph into for GNN\n",
        "    \"clusters_per_batch\": 5,  # Number of clusters to sample per batch for GNN\n",
        "    \"cluster_method\": \"metis\",  # Options: 'metis', 'random', 'kmeans'\n",
        "    \"solver\": \"cg\"\n",
        "}\n",
        "\n",
        "# Prepare output folder\n",
        "if config[\"name\"]:\n",
        "    folder = f\"results/{config['name']}\"\n",
        "else:\n",
        "    folder = datetime.datetime.now().strftime(\"results/%Y-%m-%d_%H-%M-%S\")\n",
        "if config[\"save\"]:\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "    save_dict_to_file(config, os.path.join(folder, \"config.json\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **DATALOADERS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_dataloader(dataset, n=0, batch_size=1, spd=True, mode=\"train\", size=None, graph=True):\n",
        "    # Setup datasets\n",
        "\n",
        "    if dataset == \"random\":\n",
        "        data = FolderDataset(f\"./dataset/{mode}/\", n, size=size, graph=graph)\n",
        "    else:\n",
        "        raise NotImplementedError(\"Dataset not implemented, Available: random\")\n",
        "\n",
        "    # Data Loaders\n",
        "    if mode == \"train\":\n",
        "        dataloader = DataLoader(data, batch_size=batch_size, shuffle=True)\n",
        "    else:\n",
        "        dataloader = DataLoader(data, batch_size=1, shuffle=False)\n",
        "\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ClusterGCNSampler:\n",
        "    def __init__(self, num_clusters, cluster_method='metis'):\n",
        "        self.num_clusters = num_clusters\n",
        "        self.cluster_method = cluster_method\n",
        "        self.partitions = {}  # Cache for partitioned clusters\n",
        "        self.cluster_nodes = {}  # Cache for nodes in each cluster\n",
        "\n",
        "    def _get_graph_hash(self, data):\n",
        "        \"\"\"\n",
        "        Create a stable hash for the graph data that doesn't depend on object identity\n",
        "        \"\"\"\n",
        "        # Create hash based on graph structure, not object identity\n",
        "        edge_hash = hash(tuple(data.edge_index.flatten().tolist()))\n",
        "        node_hash = hash(data.num_nodes)\n",
        "        \n",
        "        # Include matrix hash if present\n",
        "        if hasattr(data, 'matrix'):\n",
        "            if hasattr(data.matrix, '_indices'):  # Sparse tensor\n",
        "                matrix_hash = hash((tuple(data.matrix._indices().flatten().tolist()), \n",
        "                                  tuple(data.matrix._values().tolist())))\n",
        "            else:  # Dense tensor\n",
        "                matrix_hash = hash(tuple(data.matrix.flatten().tolist()))\n",
        "        else:\n",
        "            matrix_hash = 0\n",
        "        \n",
        "        return hash((edge_hash, node_hash, matrix_hash))\n",
        "\n",
        "    def partition_graph_once(self, data):\n",
        "        \"\"\"\n",
        "        Partition the graph once and cache the results using stable graph hash\n",
        "        \"\"\"\n",
        "        graph_hash = self._get_graph_hash(data)\n",
        "        \n",
        "        if graph_hash in self.partitions:\n",
        "            return self.partitions[graph_hash], self.cluster_nodes[graph_hash]\n",
        "        \n",
        "        edge_index = data.edge_index\n",
        "        num_nodes = data.num_nodes\n",
        "\n",
        "        # Create adjacency list for clustering\n",
        "        if self.cluster_method == 'metis':\n",
        "            # Convert to format expected by METIS\n",
        "            adjacency_list = [[] for _ in range(num_nodes)]\n",
        "            for i in range(edge_index.size(1)):\n",
        "                src, dst = edge_index[0, i].item(), edge_index[1, i].item()\n",
        "                if src != dst:  # Avoid self-loops for clustering\n",
        "                    adjacency_list[src].append(dst)\n",
        "                    adjacency_list[dst].append(src)\n",
        "\n",
        "            try:\n",
        "                # Use METIS for graph partitioning\n",
        "                _, node_clusters = metis.part_graph(adjacency_list, self.num_clusters)\n",
        "                node_clusters = torch.tensor(node_clusters, dtype=torch.long)\n",
        "            except Exception as e:\n",
        "                # Fallback to random partitioning if METIS fails\n",
        "                print(f\"METIS failed: {e}, falling back to random partitioning\")\n",
        "                node_clusters = torch.randint(0, self.num_clusters, (num_nodes,))\n",
        "        else:\n",
        "            # Random partitioning fallback\n",
        "            node_clusters = torch.randint(0, self.num_clusters, (num_nodes,))\n",
        "\n",
        "        # Pre-compute which nodes belong to each cluster\n",
        "        cluster_to_nodes = {}\n",
        "        for cluster_id in range(self.num_clusters):\n",
        "            mask = (node_clusters == cluster_id)\n",
        "            cluster_nodes = torch.where(mask)[0]\n",
        "            if len(cluster_nodes) > 0:  # Only store non-empty clusters\n",
        "                cluster_to_nodes[cluster_id] = cluster_nodes\n",
        "\n",
        "        # Cache the results using stable hash\n",
        "        self.partitions[graph_hash] = node_clusters\n",
        "        self.cluster_nodes[graph_hash] = cluster_to_nodes\n",
        "        \n",
        "        return node_clusters, cluster_to_nodes\n",
        "\n",
        "    def sample_clusters(self, data, clusters_per_batch):\n",
        "        \"\"\"\n",
        "        Sample a subset of clusters and return the induced subgraph\n",
        "        \"\"\"\n",
        "        # Get or compute partitions (only done once per unique graph)\n",
        "        node_clusters, cluster_to_nodes = self.partition_graph_once(data)\n",
        "\n",
        "        # Get available clusters (only non-empty ones)\n",
        "        available_clusters = list(cluster_to_nodes.keys())\n",
        "        \n",
        "        if len(available_clusters) == 0:\n",
        "            raise ValueError(\"No valid clusters found\")\n",
        "        \n",
        "        # Randomly sample clusters for this batch\n",
        "        if len(available_clusters) <= clusters_per_batch:\n",
        "            selected_clusters = available_clusters\n",
        "        else:\n",
        "            # Use numpy for faster random sampling\n",
        "            selected_clusters = np.random.choice(\n",
        "                available_clusters, \n",
        "                size=clusters_per_batch, \n",
        "                replace=False\n",
        "            ).tolist()\n",
        "\n",
        "        # Collect all nodes from selected clusters\n",
        "        selected_nodes_list = []\n",
        "        for cluster_id in selected_clusters:\n",
        "            selected_nodes_list.append(cluster_to_nodes[cluster_id])\n",
        "        \n",
        "        selected_nodes = torch.cat(selected_nodes_list)\n",
        "\n",
        "        # Extract subgraph\n",
        "        edge_index, edge_attr = subgraph(\n",
        "            selected_nodes,\n",
        "            data.edge_index,\n",
        "            edge_attr=data.edge_attr if hasattr(data, 'edge_attr') else None,\n",
        "            relabel_nodes=True,\n",
        "            num_nodes=data.num_nodes\n",
        "        )\n",
        "\n",
        "        # Create new data object for the subgraph\n",
        "        subgraph_data = Data(\n",
        "            edge_index=edge_index,\n",
        "            num_nodes=len(selected_nodes)\n",
        "        )\n",
        "\n",
        "        # Copy relevant attributes\n",
        "        if hasattr(data, 'x') and data.x is not None:\n",
        "            subgraph_data.x = data.x[selected_nodes]\n",
        "\n",
        "        if hasattr(data, 'edge_attr') and data.edge_attr is not None:\n",
        "            subgraph_data.edge_attr = edge_attr\n",
        "\n",
        "        # For your preconditioner application, you'll need to handle the matrix\n",
        "        if hasattr(data, 'matrix'):\n",
        "            # Extract the submatrix corresponding to selected nodes\n",
        "            subgraph_data.matrix = data.matrix[selected_nodes][:, selected_nodes]\n",
        "\n",
        "        # Store mapping for reconstruction if needed\n",
        "        subgraph_data.original_nodes = selected_nodes\n",
        "        subgraph_data.node_mapping = {new_idx: old_idx.item()\n",
        "                                    for new_idx, old_idx in enumerate(selected_nodes)}\n",
        "        subgraph_data.selected_clusters = selected_clusters\n",
        "\n",
        "        return subgraph_data\n",
        "\n",
        "class ClusterGCNDataLoader:\n",
        "    \"\"\"\n",
        "    Custom DataLoader that uses ClusterGCN sampling with preprocessing partitioning\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset, batch_size, num_clusters, clusters_per_batch,\n",
        "                 cluster_method='metis', shuffle=True, load_from_cache=False):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.sampler = ClusterGCNSampler(num_clusters, cluster_method)\n",
        "        self.clusters_per_batch = clusters_per_batch\n",
        "        self.load_from_cache = load_from_cache\n",
        "        \n",
        "        if not self.load_from_cache:\n",
        "            self._precompute_all_partitions()\n",
        "        else: \n",
        "            print(\"load_from_cache=True: skipping Graph partitioning step...\")\n",
        "\n",
        "    def _precompute_all_partitions(self):\n",
        "        \"\"\"\n",
        "        Precompute all graph partitions during initialization\n",
        "        \"\"\"\n",
        "        print(f\"Preprocessing: Computing graph partitions for {len(self.dataset)} graphs...\")\n",
        "        start_time = time.time()\n",
        "        \n",
        "        for i, data in tqdm(enumerate(self.dataset)):\n",
        "            self.sampler.partition_graph_once(data)\n",
        "            \n",
        "            # Progress reporting\n",
        "            if len(self.dataset) > 100:\n",
        "                if (i + 1) % max(1, len(self.dataset) // 20) == 0:  # Report every 5%\n",
        "                    elapsed = time.time() - start_time\n",
        "                    progress = (i + 1) / len(self.dataset)\n",
        "                    eta = elapsed / progress - elapsed if progress > 0 else 0\n",
        "                    print(f\"  Progress: {i + 1}/{len(self.dataset)} ({progress*100:.1f}%) - \"\n",
        "                          f\"Elapsed: {elapsed:.1f}s - ETA: {eta:.1f}s\")\n",
        "            elif (i + 1) % 10 == 0 or i == len(self.dataset) - 1:\n",
        "                print(f\"  Partitioned {i + 1}/{len(self.dataset)} graphs\")\n",
        "        \n",
        "        total_time = time.time() - start_time\n",
        "        print(f\"Preprocessing completed in {total_time:.2f} seconds\")\n",
        "        print(f\"Average partitioning time: {total_time/len(self.dataset)*1000:.2f}ms per graph\")\n",
        "\n",
        "    def __iter__(self):\n",
        "        indices = list(range(len(self.dataset)))\n",
        "        if self.shuffle:\n",
        "            indices = torch.randperm(len(indices)).tolist()\n",
        "\n",
        "        for i in range(0, len(indices), self.batch_size):\n",
        "            batch_indices = indices[i:i + self.batch_size]\n",
        "            batch_data = []\n",
        "\n",
        "            for idx in batch_indices:\n",
        "                data = self.dataset[idx]\n",
        "                # Sample clusters for this graph (partitioning already done)\n",
        "                subgraph_data = self.sampler.sample_clusters(data, self.clusters_per_batch)\n",
        "                batch_data.append(subgraph_data)\n",
        "\n",
        "            if len(batch_data) == 1:\n",
        "                yield batch_data[0]\n",
        "            else:\n",
        "                yield Batch.from_data_list(batch_data)\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.dataset) + self.batch_size - 1) // self.batch_size\n",
        "\n",
        "def get_cluster_dataloader(dataset, n, batch_size, spd=True, mode=\"train\", num_clusters=10,\n",
        "                           clusters_per_batch=2, size=None, cache_dir=\"./partition_cache\", load_from_cache=False):\n",
        "    \"\"\"\n",
        "    If load_from_cache=False: partitions & caches to disk, returns loader.\n",
        "    If load_from_cache=True: loads partitions & returns loader (no re-partition).\n",
        "    \"\"\"\n",
        "    print(f\"Loading dataset [{mode}]…\")\n",
        "    ds = FolderDataset(f\"./dataset/{mode}/\", n, size=size, graph=True)\n",
        "    print(f\"  → {len(ds)} graphs\")\n",
        "\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "    cache_file = os.path.join(cache_dir, f\"{mode}_n{n}_k{num_clusters}_partitions.pkl\")\n",
        "\n",
        "    if load_from_cache:\n",
        "        # 1) load pickle\n",
        "        if not os.path.exists(cache_file):\n",
        "            raise FileNotFoundError(f\"No cache found at {cache_file!r}. Check if the parameters are correct.\")\n",
        "        with open(cache_file, \"rb\") as f:\n",
        "            data = pickle.load(f)\n",
        "        parts, nodes = data[\"partitions\"], data[\"cluster_nodes\"]\n",
        "        # 2) build loader without precompute\n",
        "        loader = ClusterGCNDataLoader(\n",
        "            ds, batch_size, num_clusters, clusters_per_batch,\n",
        "            cluster_method='metis',\n",
        "            shuffle=(mode==\"train\"),\n",
        "            load_from_cache=True\n",
        "        )\n",
        "        # 3) inject caches\n",
        "        loader.sampler.partitions    = parts\n",
        "        loader.sampler.cluster_nodes = nodes\n",
        "        print(f\"✔ Loaded partitions from {cache_file}\")\n",
        "    else:\n",
        "        # build & precompute\n",
        "        loader = ClusterGCNDataLoader(\n",
        "            ds, batch_size, num_clusters, clusters_per_batch,\n",
        "            cluster_method='metis',\n",
        "            shuffle=(mode==\"train\"),\n",
        "            load_from_cache=False\n",
        "        )\n",
        "        # save to disk\n",
        "        with open(cache_file, \"wb\") as f:\n",
        "            pickle.dump({\n",
        "                \"partitions\": loader.sampler.partitions,\n",
        "                \"cluster_nodes\": loader.sampler.cluster_nodes\n",
        "            }, f)\n",
        "        print(f\"✔ Cached partitions to {cache_file}\")\n",
        "\n",
        "    print(\"DataLoader ready!\")\n",
        "    return loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **MODELS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "33unmKPgQAC1"
      },
      "outputs": [],
      "source": [
        "class GraphNet(nn.Module):\n",
        "    # Follows roughly the outline of torch_geometric.nn.MessagePassing()\n",
        "    # As shown in https://github.com/deepmind/graph_nets\n",
        "    # Here is a helpful python implementation:\n",
        "    # https://github.com/NVIDIA/GraphQSat/blob/main/gqsat/models.py\n",
        "    # Also allows multirgaph GNN via edge_2_features\n",
        "    def __init__(self, node_features, edge_features, global_features=0, hidden_size=0,\n",
        "                 aggregate=\"mean\", activation=\"relu\", skip_connection=False, edge_features_out=None):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # different aggregation functions\n",
        "        if aggregate == \"sum\":\n",
        "            self.aggregate = aggr.SumAggregation()\n",
        "        elif aggregate == \"mean\":\n",
        "            self.aggregate = aggr.MeanAggregation()\n",
        "        elif aggregate == \"max\":\n",
        "            self.aggregate = aggr.MaxAggregation()\n",
        "        elif aggregate == \"softmax\":\n",
        "            self.aggregate = aggr.SoftmaxAggregation(learn=True)\n",
        "        else:\n",
        "            raise NotImplementedError(f\"Aggregation '{aggregate}' not implemented\")\n",
        "\n",
        "        self.global_aggregate = aggr.MeanAggregation()\n",
        "\n",
        "        add_edge_fs = 1 if skip_connection else 0\n",
        "        edge_features_out = edge_features if edge_features_out is None else edge_features_out\n",
        "\n",
        "        # Graph Net Blocks (see https://arxiv.org/pdf/1806.01261.pdf)\n",
        "        self.edge_block = MLP([global_features + (edge_features + add_edge_fs) + (2 * node_features),\n",
        "                               hidden_size,\n",
        "                               edge_features_out],\n",
        "                              activation=activation)\n",
        "\n",
        "        self.node_block = MLP([global_features + edge_features_out + node_features,\n",
        "                               hidden_size,\n",
        "                               node_features],\n",
        "                              activation=activation)\n",
        "\n",
        "        # optional set of blocks for global GNN\n",
        "        self.global_block = None\n",
        "        if global_features > 0:\n",
        "            self.global_block = MLP([edge_features_out + node_features + global_features,\n",
        "                                     hidden_size,\n",
        "                                     global_features],\n",
        "                                    activation=activation)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr, g=None):\n",
        "        row, col = edge_index\n",
        "\n",
        "        if self.global_block is not None:\n",
        "            assert g is not None, \"Need global features for global block\"\n",
        "\n",
        "            # run the edge update and aggregate features\n",
        "            edge_embedding = self.edge_block(torch.cat([torch.ones(x[row].shape[0], 1, device=x.device) * g,\n",
        "                                                        x[row], x[col], edge_attr], dim=1))\n",
        "            aggregation = self.aggregate(edge_embedding, row)\n",
        "\n",
        "\n",
        "            agg_features = torch.cat([torch.ones(x.shape[0], 1, device=x.device) * g, x, aggregation], dim=1)\n",
        "            node_embeddings = self.node_block(agg_features)\n",
        "\n",
        "            # aggregate over all edges and nodes (always mean)\n",
        "            mp_global_aggr = g\n",
        "            edge_aggregation_global = self.global_aggregate(edge_embedding)\n",
        "            node_aggregation_global = self.global_aggregate(node_embeddings)\n",
        "\n",
        "            # compute the new global embedding\n",
        "            # the old global feature is part of mp_global_aggr\n",
        "            global_embeddings = self.global_block(torch.cat([node_aggregation_global,\n",
        "                                                             edge_aggregation_global,\n",
        "                                                             mp_global_aggr], dim=1))\n",
        "\n",
        "            return edge_embedding, node_embeddings, global_embeddings\n",
        "\n",
        "        else:\n",
        "            # update edge features and aggregate\n",
        "            edge_embedding = self.edge_block(torch.cat([x[row], x[col], edge_attr], dim=1))\n",
        "            aggregation = self.aggregate(edge_embedding, row)\n",
        "            agg_features = torch.cat([x, aggregation], dim=1)\n",
        "            # update node features\n",
        "            node_embeddings = self.node_block(agg_features)\n",
        "            return edge_embedding, node_embeddings, None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "HZBjB3GUPtrV"
      },
      "outputs": [],
      "source": [
        "class NeuralIF(nn.Module):\n",
        "    # Neural Incomplete factorization\n",
        "    def __init__(self, drop_tol=0, **kwargs) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.global_features = kwargs[\"global_features\"]\n",
        "        self.latent_size = kwargs[\"latent_size\"]\n",
        "        # node features are augmented with local degree profile\n",
        "        self.augment_node_features = kwargs[\"augment_nodes\"]\n",
        "\n",
        "        num_node_features = 8 if self.augment_node_features else 1\n",
        "        message_passing_steps = kwargs[\"message_passing_steps\"]\n",
        "\n",
        "        # edge feature representation in the latent layers\n",
        "        edge_features = kwargs.get(\"edge_features\", 1)\n",
        "\n",
        "        self.skip_connections = kwargs[\"skip_connections\"]\n",
        "\n",
        "        self.mps = torch.nn.ModuleList()\n",
        "        for l in range(message_passing_steps):\n",
        "            # skip connections are added to all layers except the first one\n",
        "            self.mps.append(MP_Block(skip_connections=self.skip_connections,\n",
        "                                     first=l==0,\n",
        "                                     last=l==(message_passing_steps-1),\n",
        "                                     edge_features=edge_features,\n",
        "                                     node_features=num_node_features,\n",
        "                                     global_features=self.global_features,\n",
        "                                     hidden_size=self.latent_size,\n",
        "                                     activation=kwargs[\"activation\"],\n",
        "                                     aggregate=kwargs[\"aggregate\"]))\n",
        "\n",
        "        # node decodings\n",
        "        self.node_decoder = MLP([num_node_features, self.latent_size, 1]) if kwargs[\"decode_nodes\"] else None\n",
        "\n",
        "        # diag-aggregation for normalization of rows\n",
        "        self.normalize_diag = kwargs[\"normalize_diag\"] if \"normalize_diag\" in kwargs else False\n",
        "        self.diag_aggregate = aggr.SumAggregation()\n",
        "\n",
        "        # normalization\n",
        "        self.graph_norm = pyg.norm.GraphNorm(num_node_features) if (\"graph_norm\" in kwargs and kwargs[\"graph_norm\"]) else None\n",
        "\n",
        "        # drop tolerance and additional fill-ins and more sparsity\n",
        "        self.tau = drop_tol\n",
        "        self.two = kwargs.get(\"two_hop\", False)\n",
        "\n",
        "    def forward(self, data):\n",
        "        # ! data could be batched here...(not implemented)\n",
        "\n",
        "        if self.augment_node_features:\n",
        "            data = augment_features(data, skip_rhs=True)\n",
        "\n",
        "        # add additional edges to the data\n",
        "        if self.two:\n",
        "            data = TwoHop()(data)\n",
        "\n",
        "        # * in principle it is possible to integrate reordering here.\n",
        "\n",
        "        data = ToLowerTriangular()(data)\n",
        "\n",
        "        # get the input data\n",
        "        edge_embedding = data.edge_attr\n",
        "        l_index = data.edge_index\n",
        "\n",
        "        if self.graph_norm is not None:\n",
        "            node_embedding = self.graph_norm(data.x, batch=data.batch)\n",
        "        else:\n",
        "            node_embedding = data.x\n",
        "\n",
        "        # copy the input data (only edges of original matrix A)\n",
        "        a_edges = edge_embedding.clone()\n",
        "\n",
        "        if self.global_features > 0:\n",
        "            global_features = torch.zeros((1, self.global_features), device=data.x.device, requires_grad=False)\n",
        "            # feature ideas: nnz, 1-norm, inf-norm col/row var, min/max variability, avg distances to nnz\n",
        "        else:\n",
        "            global_features = None\n",
        "\n",
        "        # compute the output of the network\n",
        "        for i, layer in enumerate(self.mps):\n",
        "            if i != 0 and self.skip_connections:\n",
        "                edge_embedding = torch.cat([edge_embedding, a_edges], dim=1)\n",
        "\n",
        "            edge_embedding, node_embedding, global_features = layer(node_embedding, l_index, edge_embedding, global_features)\n",
        "\n",
        "        # transform the output into a matrix\n",
        "        return self.transform_output_matrix(node_embedding, l_index, edge_embedding, a_edges)\n",
        "\n",
        "    def transform_output_matrix(self, node_x, edge_index, edge_values, a_edges):\n",
        "        # force diagonal to be positive\n",
        "        diag = edge_index[0] == edge_index[1]\n",
        "\n",
        "        # normalize diag such that it has zero residual\n",
        "        if self.normalize_diag:\n",
        "            # copy the diag of matrix A\n",
        "            a_diag = a_edges[diag]\n",
        "\n",
        "            # compute the row norm\n",
        "            square_values = torch.pow(edge_values, 2)\n",
        "            aggregated = self.diag_aggregate(square_values, edge_index[0])\n",
        "\n",
        "            # now, we renormalize the edge values such that they are the square root of the original value...\n",
        "            edge_values = torch.sqrt(a_diag[edge_index[0]]) * edge_values / torch.sqrt(aggregated[edge_index[0]])\n",
        "\n",
        "        else:\n",
        "            # otherwise, just take the edge values as they are...\n",
        "            # but take the square root as it is numerically better\n",
        "            # edge_values[diag] = torch.exp(edge_values[diag])\n",
        "            edge_values[diag] = torch.sqrt(torch.exp(edge_values[diag]))\n",
        "\n",
        "        # node decoder\n",
        "        node_output = self.node_decoder(node_x).squeeze() if self.node_decoder is not None else None\n",
        "\n",
        "        # ! this if should only be activated when the model is in production!!\n",
        "        if torch.is_inference_mode_enabled():\n",
        "\n",
        "            # we can decide to remove small elements during inference from the preconditioner matrix\n",
        "            if self.tau != 0:\n",
        "                small_value = (torch.abs(edge_values) <= self.tau).squeeze()\n",
        "\n",
        "                # small value and not diagonal\n",
        "                elems = torch.logical_and(small_value, torch.logical_not(diag))\n",
        "\n",
        "                # might be able to do this easily!\n",
        "                edge_values[elems] = 0\n",
        "\n",
        "                # remove zeros from the sparse representation\n",
        "                filt = (edge_values != 0).squeeze()\n",
        "                edge_values = edge_values[filt]\n",
        "                edge_index = edge_index[:, filt]\n",
        "\n",
        "            # ! this is the way to go!!\n",
        "            # Doing pytorch -> scipy -> numml is a lot faster than pytorch -> numml on CPU\n",
        "            # On GPU it is faster to go to pytorch -> numml -> CPU\n",
        "\n",
        "            # convert to scipy sparse matrix\n",
        "            # m = to_scipy_sparse_matrix(edge_index, matrix_values)\n",
        "            m = torch.sparse_coo_tensor(edge_index, edge_values.squeeze(),\n",
        "                                        size=(node_x.size()[0], node_x.size()[0]))\n",
        "                                        # type=torch.double)\n",
        "\n",
        "            # produce L and U seperatly\n",
        "            l = SparseCSRTensor(m)\n",
        "            u = SparseCSRTensor(m.T)\n",
        "\n",
        "            return l, u, node_output\n",
        "\n",
        "        else:\n",
        "            # For training and testing (computing regular losses for examples.)\n",
        "            # does not need to be performance optimized!\n",
        "            # use torch sparse directly\n",
        "            t = torch.sparse_coo_tensor(edge_index, edge_values.squeeze(),\n",
        "                                        size=(node_x.size()[0], node_x.size()[0]))\n",
        "\n",
        "            # normalized l1 norm is best computed here!\n",
        "            # l2_nn = torch.linalg.norm(edge_values, ord=2)\n",
        "            l1_penalty = torch.sum(torch.abs(edge_values)) / len(edge_values)\n",
        "\n",
        "            return t, l1_penalty, node_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "19DkjzdMP-q0"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, width, layer_norm=False, activation=\"relu\", activate_final=False):\n",
        "        super().__init__()\n",
        "        width = list(filter(lambda x: x > 0, width))\n",
        "        assert len(width) >= 2, \"Need at least one layer in the network!\"\n",
        "\n",
        "        lls = nn.ModuleList()\n",
        "        for k in range(len(width)-1):\n",
        "            lls.append(nn.Linear(width[k], width[k+1], bias=True))\n",
        "            if k != (len(width)-2) or activate_final:\n",
        "                if activation == \"relu\":\n",
        "                    lls.append(nn.ReLU())\n",
        "                elif activation == \"tanh\":\n",
        "                    lls.append(nn.Tanh())\n",
        "                elif activation == \"leakyrelu\":\n",
        "                    lls.append(nn.LeakyReLU())\n",
        "                elif activation == \"sigmoid\":\n",
        "                    lls.append(nn.Sigmoid())\n",
        "                else:\n",
        "                    raise NotImplementedError(f\"Activation '{activation}' not implemented\")\n",
        "\n",
        "        if layer_norm:\n",
        "            lls.append(nn.LayerNorm(width[-1]))\n",
        "\n",
        "        self.m = nn.Sequential(*lls)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.m(x)\n",
        "\n",
        "class ToLowerTriangular(torch_geometric.transforms.BaseTransform):\n",
        "    def __init__(self, inplace=False):\n",
        "        self.inplace = inplace\n",
        "\n",
        "    def __call__(self, data, order=None):\n",
        "        if not self.inplace:\n",
        "            data = data.clone()\n",
        "\n",
        "        # TODO: if order is given use that one instead\n",
        "        if order is not None:\n",
        "            raise NotImplementedError(\"Custom ordering not yet implemented...\")\n",
        "\n",
        "        # transform the data into lower triag graph\n",
        "        # this should be a data transformation (maybe?)\n",
        "        rows, cols = data.edge_index[0], data.edge_index[1]\n",
        "        fil = cols <= rows\n",
        "        l_index = data.edge_index[:, fil]\n",
        "        edge_embedding = data.edge_attr[fil]\n",
        "\n",
        "        data.edge_index, data.edge_attr = l_index, edge_embedding\n",
        "        return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "jVDRs1QIP5Lw"
      },
      "outputs": [],
      "source": [
        "class MP_Block(nn.Module):\n",
        "    # L@L.T matrix multiplication graph layer\n",
        "    # Aligns the computation of L@L.T - A with the learned updates\n",
        "    def __init__(self, skip_connections, first, last, edge_features, node_features, global_features, hidden_size, **kwargs) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        # first and second aggregation\n",
        "        if \"aggregate\" in kwargs and kwargs[\"aggregate\"] is not None:\n",
        "            aggr = kwargs[\"aggregate\"] if len(kwargs[\"aggregate\"]) == 2 else kwargs[\"aggregate\"] * 2\n",
        "        else:\n",
        "            aggr = [\"mean\", \"sum\"]\n",
        "\n",
        "        act = kwargs[\"activation\"] if \"activation\" in kwargs else \"relu\"\n",
        "\n",
        "        edge_features_in = 1 if first else edge_features\n",
        "        edge_features_out = 1 if last else edge_features\n",
        "\n",
        "        # We use 2 graph nets in order to operate on the upper and lower triangular parts of the matrix\n",
        "        self.l1 = GraphNet(node_features=node_features, edge_features=edge_features_in, global_features=global_features,\n",
        "                           hidden_size=hidden_size, skip_connection=(not first and skip_connections),\n",
        "                           aggregate=aggr[0], activation=act, edge_features_out=edge_features)\n",
        "\n",
        "        self.l2 = GraphNet(node_features=node_features, edge_features=edge_features, global_features=global_features,\n",
        "                           hidden_size=hidden_size, aggregate=aggr[1], activation=act, edge_features_out=edge_features_out)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr, global_features):\n",
        "        edge_embedding, node_embeddings, global_features = self.l1(x, edge_index, edge_attr, g=global_features)\n",
        "\n",
        "        # flip row and column indices\n",
        "        edge_index = torch.stack([edge_index[1], edge_index[0]], dim=0)\n",
        "        edge_embedding, node_embeddings, global_features = self.l2(node_embeddings, edge_index, edge_embedding, g=global_features)\n",
        "\n",
        "        return edge_embedding, node_embeddings, global_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsvyF-64N506"
      },
      "source": [
        "# **VALIDATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Preconditioner:\n",
        "    def __init__(self, A, **kwargs):\n",
        "        self.breakdown = False\n",
        "        self.nnz = 0\n",
        "        self.time = 0\n",
        "        self.n = kwargs.get(\"n\", 0)\n",
        "\n",
        "    def timed_setup(self, A, **kwargs):\n",
        "        start = time_function()\n",
        "        self.setup(A, **kwargs)\n",
        "        stop = time_function()\n",
        "        self.time = stop - start\n",
        "\n",
        "    def get_inverse(self):\n",
        "        ones = torch.ones(self.n)\n",
        "        offset = torch.zeros(1).to(torch.int64)\n",
        "\n",
        "        I = torch.sparse.spdiags(ones, offset, (self.n, self.n))\n",
        "        I = I.to(torch.float64)\n",
        "\n",
        "        return I\n",
        "\n",
        "    def get_p_matrix(self):\n",
        "        return self.get_inverse()\n",
        "\n",
        "    def check_breakdown(self, P):\n",
        "        if np.isnan(np.min(P)):\n",
        "            self.breakdown = True\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return x\n",
        "\n",
        "class LearnedPreconditioner(Preconditioner):\n",
        "    def __init__(self, data, model, **kwargs):\n",
        "        super().__init__(data, **kwargs)\n",
        "\n",
        "        self.model = model\n",
        "        self.spd = isinstance(model, NeuralIF)\n",
        "\n",
        "        self.timed_setup(data, **kwargs)\n",
        "\n",
        "        if self.spd:\n",
        "            self.nnz = self.L.nnz\n",
        "        else:\n",
        "            self.nnz = self.L.nnz + self.U.nnz - data.x.shape[0]\n",
        "\n",
        "    def setup(self, data, **kwargs):\n",
        "        L, U, _ = self.model(data)\n",
        "\n",
        "        self.L = L.to(\"cpu\").to(torch.float64)\n",
        "        self.U = U.to(\"cpu\").to(torch.float64)\n",
        "\n",
        "    def get_inverse(self):\n",
        "        L_inv = torch.inverse(self.L.to_dense())\n",
        "        U_inv = torch.inverse(self.U.to_dense())\n",
        "\n",
        "        return U_inv@L_inv\n",
        "\n",
        "    def get_p_matrix(self):\n",
        "        return self.L@self.U\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return fb_solve(self.L, self.U, x, unit_upper=not self.spd)\n",
        "\n",
        "\n",
        "def fb_solve(L, U, r, unit_lower=False, unit_upper=False):\n",
        "    y = L.solve_triangular(upper=False, unit=unit_lower, b=r)\n",
        "    z = U.solve_triangular(upper=True, unit=unit_upper, b=y)\n",
        "    return z\n",
        "\n",
        "\n",
        "def fb_solve_joint(LU, r):\n",
        "    # Note: solve triangular ignores the values in lower/upper triangle\n",
        "    y = LU.solve_triangular(upper=False, unit=False, b=r)\n",
        "    z = LU.solve_triangular(upper=True, unit=False, b=y)\n",
        "    return z\n",
        "\n",
        "time_function = lambda: time.perf_counter()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "lmG0zvFyN3vj"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def validate(model, validation_loader, solve=False, solver=\"cg\", **kwargs):\n",
        "    model.eval()\n",
        "\n",
        "    acc_loss = 0.0\n",
        "    num_loss = 0\n",
        "    acc_solver_iters = 0.0\n",
        "\n",
        "    for i, data in enumerate(validation_loader):\n",
        "        data = data.to(device)\n",
        "\n",
        "        # construct problem data\n",
        "        A, b = graph_to_matrix(data)\n",
        "\n",
        "        # run conjugate gradient method\n",
        "        # this requires the learned preconditioner to be reasonably good!\n",
        "        if solve:\n",
        "            # run CG on CPU\n",
        "            with torch.inference_mode():\n",
        "                preconditioner = LearnedPreconditioner(data, model)\n",
        "\n",
        "            A = A.to(\"cpu\").to(torch.float64)\n",
        "            b = b.to(\"cpu\").to(torch.float64)\n",
        "            x_init = None\n",
        "\n",
        "            solver_start = time.time()\n",
        "\n",
        "            if solver == \"cg\":\n",
        "                l, x_hat = preconditioned_conjugate_gradient(A.to(\"cpu\"), b.to(\"cpu\"), M=preconditioner,\n",
        "                                                             x0=x_init, rtol=1e-6, max_iter=1_000)\n",
        "                \n",
        "                # l, x_hat = preconditioned_conjugate_gradient(A.to(\"cpu\"), b.to(\"cpu\"), M=None,\n",
        "                #                                              x0=x_init, rtol=1e-6, max_iter=1_000)\n",
        "            elif solver == \"gmres\":\n",
        "                l, x_hat = gmres(A, b, M=preconditioner, x0=x_init, atol=1e-6, max_iter=1_000, left=False)\n",
        "            else:\n",
        "                raise NotImplementedError(\"Solver not implemented choose between CG and GMRES!\")\n",
        "\n",
        "            solver_stop = time.time()\n",
        "\n",
        "            # Measure preconditioning performance\n",
        "            solver_time = (solver_stop - solver_start)\n",
        "            acc_solver_iters += len(l) - 1\n",
        "\n",
        "        else:\n",
        "            output, _, _ = model(data)\n",
        "\n",
        "            # Here, we compute the loss using the full forbenius norm (no estimator)\n",
        "            # l = frobenius_loss(output, A)\n",
        "\n",
        "            l = loss(data, output, config=None) ##USe the default loss instead of the frobenius loss\n",
        "\n",
        "            acc_loss += l.item()\n",
        "            num_loss += 1\n",
        "\n",
        "    if solve:\n",
        "        # print(f\"Smallest eigenvalue: {dist[0]}\")\n",
        "        print(f\"Validation\\t iterations:\\t{acc_solver_iters / len(validation_loader):.2f}\")\n",
        "        return acc_solver_iters / len(validation_loader)\n",
        "\n",
        "    else:\n",
        "        print(f\"Validation loss:\\t{acc_loss / num_loss:.2f}\")\n",
        "        return acc_loss / len(validation_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsVE6R9SOLkT"
      },
      "source": [
        "# **MODEL TRAINING**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **BASELINE NEURALIF**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4ci3LbMOPEG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number params in model: 460\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "136it [00:10, 12.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 \t loss: 1097.0139586504768 \t time: 10.731323720000091\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "136it [00:10, 12.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 \t loss: 471.38088181439565 \t time: 10.981040607999603\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "136it [00:10, 12.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 \t loss: 431.88145850686465 \t time: 10.583723503000328\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "136it [00:10, 12.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 \t loss: 419.03057457419004 \t time: 10.782060632000139\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "136it [00:10, 12.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 \t loss: 407.4191353741814 \t time: 10.870200481999746\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "136it [00:10, 13.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 \t loss: 393.6336337818819 \t time: 10.225598386000001\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "136it [00:10, 12.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 \t loss: 384.4011674768784 \t time: 10.494448142999772\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "48it [02:06, 17.32s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation\t iterations:\t276.50\n",
            "276.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "136it [02:13,  1.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8 \t loss: 376.21987129660215 \t time: 133.48633064900014\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "136it [00:11, 12.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9 \t loss: 370.46222080903897 \t time: 11.311065273999702\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "136it [00:11, 12.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10 \t loss: 367.51837360157685 \t time: 11.021956188999866\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "136it [00:11, 12.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11 \t loss: 365.1175483254825 \t time: 11.124394987999949\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "136it [00:10, 12.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12 \t loss: 362.9482177285587 \t time: 10.883757287000208\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "136it [00:10, 12.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13 \t loss: 361.38887382956113 \t time: 10.693607877999966\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "136it [00:10, 12.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14 \t loss: 360.9041445115033 \t time: 10.750646944000437\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "96it [02:05, 16.72s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation\t iterations:\t268.40\n",
            "268.4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "136it [02:08,  1.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15 \t loss: 359.74800760605757 \t time: 128.8257134720002\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "136it [00:11, 12.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16 \t loss: 357.1126677569221 \t time: 11.249121930000001\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "136it [00:11, 12.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17 \t loss: 356.74992280847886 \t time: 11.139036469000075\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "136it [00:10, 12.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18 \t loss: 355.80646829044116 \t time: 10.483333674999813\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "136it [00:10, 12.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19 \t loss: 356.0829209720387 \t time: 10.928011820999927\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "136it [00:10, 12.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20 \t loss: 354.3644451814539 \t time: 10.673291319000327\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "136it [00:10, 12.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21 \t loss: 354.01501554601333 \t time: 10.717812395000237\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "136it [00:10, 12.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22 \t loss: 353.87880661908315 \t time: 10.642187573999763\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "6it [00:32,  5.38s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 100\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# Do validation after 100 updates (to support big datasets)\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[38;5;66;03m# convergence is expected to be pretty fast...\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (total_it + \u001b[32m1\u001b[39m) % \u001b[32m1000\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m     98\u001b[39m \n\u001b[32m     99\u001b[39m     \u001b[38;5;66;03m# start with cg-checks after 5 iterations\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m     val_its = validate(model, validation_loader, solve=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    101\u001b[39m                         solver=\u001b[33m\"\u001b[39m\u001b[33mgmres\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gmres \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcg\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    103\u001b[39m     \u001b[38;5;66;03m# use scheduler\u001b[39;00m\n\u001b[32m    104\u001b[39m     \u001b[38;5;66;03m# if config[\"scheduler\"]:\u001b[39;00m\n\u001b[32m    105\u001b[39m     \u001b[38;5;66;03m#    scheduler.step(val_loss)\u001b[39;00m\n\u001b[32m    107\u001b[39m     logger.log_val(\u001b[38;5;28;01mNone\u001b[39;00m, val_its)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/GML_proj/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mvalidate\u001b[39m\u001b[34m(model, validation_loader, solve, solver, **kwargs)\u001b[39m\n\u001b[32m     26\u001b[39m solver_start = time.time()\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m solver == \u001b[33m\"\u001b[39m\u001b[33mcg\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     l, x_hat = preconditioned_conjugate_gradient(A.to(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m), b.to(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m), M=preconditioner,\n\u001b[32m     30\u001b[39m                                                  x0=x_init, rtol=\u001b[32m1e-6\u001b[39m, max_iter=\u001b[32m1_000\u001b[39m)\n\u001b[32m     32\u001b[39m     \u001b[38;5;66;03m# l, x_hat = preconditioned_conjugate_gradient(A.to(\"cpu\"), b.to(\"cpu\"), M=None,\u001b[39;00m\n\u001b[32m     33\u001b[39m     \u001b[38;5;66;03m#                                              x0=x_init, rtol=1e-6, max_iter=1_000)\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m solver == \u001b[33m\"\u001b[39m\u001b[33mgmres\u001b[39m\u001b[33m\"\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/TU_Delft/CS4350/proj/GML-Project/krylov/cg.py:73\u001b[39m, in \u001b[36mpreconditioned_conjugate_gradient\u001b[39m\u001b[34m(A, b, M, x0, x_true, rtol, max_iter)\u001b[39m\n\u001b[32m     71\u001b[39m     error_i = (x_hat - x_true) \u001b[38;5;28;01mif\u001b[39;00m x_true \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m torch.zeros_like(b, requires_grad=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     72\u001b[39m     res = stopping_criterion(A, rk, b)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     errors.append((torch.inner(error_i, A\u001b[38;5;129m@error_i\u001b[39m), res))\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m errors, x_hat\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "from tqdm import tqdm\n",
        "if config[\"save\"]:\n",
        "        os.makedirs(folder, exist_ok=True)\n",
        "        save_dict_to_file(config, os.path.join(folder, \"config.json\"))\n",
        "\n",
        "# global seed-ish\n",
        "torch_geometric.seed_everything(config[\"seed\"])\n",
        "\n",
        "# args for the model\n",
        "model_args = {k: config[k] for k in [\"latent_size\", \"message_passing_steps\", \"skip_connections\",\n",
        "                                      \"augment_nodes\", \"global_features\", \"decode_nodes\",\n",
        "                                      \"normalize_diag\", \"activation\", \"aggregate\", \"graph_norm\",\n",
        "                                      \"two_hop\", \"edge_features\", \"normalize\"]\n",
        "              if k in config}\n",
        "\n",
        "# run the GMRES algorithm instead of CG (?)\n",
        "gmres = False\n",
        "\n",
        "# Create model\n",
        "\n",
        "if config[\"model\"] == \"nif\" or config[\"model\"] == \"neuralif\" or config[\"model\"] == \"inf\":\n",
        "    model = NeuralIF(**model_args)\n",
        "\n",
        "\n",
        "else:\n",
        "    raise NotImplementedError\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "print(f\"Number params in model: {count_parameters(model)}\")\n",
        "print()\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters())\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=20)\n",
        "\n",
        "# Setup datasets\n",
        "train_loader = get_dataloader(config[\"dataset\"], config[\"n\"], config[\"batch_size\"],\n",
        "                              spd=not gmres, mode=\"train\")\n",
        "\n",
        "validation_loader = get_dataloader(config[\"dataset\"], config[\"n\"], 1, spd=(not gmres), mode=\"val\")\n",
        "\n",
        "best_val = float(\"inf\")\n",
        "logger = TrainResults(folder)\n",
        "\n",
        "\n",
        "total_it = 0\n",
        "\n",
        "# Train loop\n",
        "for epoch in range(config[\"num_epochs\"]):\n",
        "    running_loss = 0.0\n",
        "    grad_norm = 0.0\n",
        "\n",
        "    start_epoch = time.perf_counter()\n",
        "\n",
        "    for it, data in tqdm(enumerate(train_loader)):\n",
        "        # increase iteration count\n",
        "        total_it += 1\n",
        "\n",
        "        # enable training mode\n",
        "        model.train()\n",
        "\n",
        "        start = time.perf_counter()\n",
        "        data = data.to(device)\n",
        "\n",
        "        output, reg, _ = model(data)\n",
        "        l = loss(output, data, c=reg, config=config[\"loss\"])\n",
        "\n",
        "        #  if reg:\n",
        "        #    l = l + config[\"regularizer\"] * reg\n",
        "\n",
        "        l.backward()\n",
        "        running_loss += l.item()\n",
        "\n",
        "        # track the gradient norm\n",
        "        if \"gradient_clipping\" in config and config[\"gradient_clipping\"]:\n",
        "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config[\"gradient_clipping\"])\n",
        "\n",
        "        else:\n",
        "            total_norm = 0.0\n",
        "\n",
        "            for p in model.parameters():\n",
        "                if p.grad is not None:\n",
        "                    param_norm = p.grad.detach().data.norm(2)\n",
        "                    total_norm += param_norm.item() ** 2\n",
        "\n",
        "            grad_norm = total_norm ** 0.5 / config[\"batch_size\"]\n",
        "\n",
        "        # update network parameters\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logger.log(l.item(), grad_norm, time.perf_counter() - start)\n",
        "\n",
        "        # Do validation after 100 updates (to support big datasets)\n",
        "        # convergence is expected to be pretty fast...\n",
        "        if (total_it + 1) % 1000 == 0:\n",
        "\n",
        "            # start with cg-checks after 5 iterations\n",
        "            val_its = validate(model, validation_loader, solve=True,\n",
        "                                solver=\"gmres\" if gmres else \"cg\")\n",
        "\n",
        "            # use scheduler\n",
        "            # if config[\"scheduler\"]:\n",
        "            #    scheduler.step(val_loss)\n",
        "\n",
        "            logger.log_val(None, val_its)\n",
        "\n",
        "            # val_perf = val_cgits if val_cgits > 0 else val_loss\n",
        "            val_perf = val_its\n",
        "            print(val_perf)\n",
        "\n",
        "            if val_perf < best_val:\n",
        "                if config[\"save\"]:\n",
        "                    torch.save(model.state_dict(), f\"{folder}/best_model.pt\")\n",
        "                best_val = val_perf\n",
        "\n",
        "    epoch_time = time.perf_counter() - start_epoch\n",
        "\n",
        "    # save model every epoch for analysis...\n",
        "    if config[\"save\"]:\n",
        "        torch.save(model.state_dict(), f\"{folder}/model_epoch{epoch+1}.pt\")\n",
        "\n",
        "    print(f\"Epoch {epoch+1} \\t loss: {1/len(train_loader) * running_loss} \\t time: {epoch_time}\")\n",
        "\n",
        "# save fully trained model\n",
        "if config[\"save\"]:\n",
        "    logger.save_results()\n",
        "    torch.save(model.to(torch.float).state_dict(), f\"{folder}/final_model.pt\")\n",
        "\n",
        "# Test the model\n",
        "# wandb.run.summary[\"validation_chol\"] = best_val\n",
        "print(\"Best validation loss:\", best_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **CLUSTER-GCN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number params in model: 460\n"
          ]
        }
      ],
      "source": [
        "# model setup\n",
        "\n",
        "from tqdm import tqdm\n",
        "if config[\"save\"]:\n",
        "        os.makedirs(folder, exist_ok=True)\n",
        "        save_dict_to_file(config, os.path.join(folder, \"config.json\"))\n",
        "\n",
        "# global seed-ish\n",
        "torch_geometric.seed_everything(config[\"seed\"])\n",
        "\n",
        "# args for the model\n",
        "model_args = {k: config[k] for k in [\"latent_size\", \"message_passing_steps\", \"skip_connections\",\n",
        "                                      \"augment_nodes\", \"global_features\", \"decode_nodes\",\n",
        "                                      \"normalize_diag\", \"activation\", \"aggregate\", \"graph_norm\",\n",
        "                                      \"two_hop\", \"edge_features\", \"normalize\"]\n",
        "              if k in config}\n",
        "\n",
        "# run the GMRES algorithm instead of CG (?)\n",
        "gmres = False\n",
        "\n",
        "# Create model\n",
        "\n",
        "if config[\"model\"] == \"nif\" or config[\"model\"] == \"neuralif\" or config[\"model\"] == \"inf\":\n",
        "    model = NeuralIF(**model_args)\n",
        "\n",
        "else:\n",
        "    raise NotImplementedError\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "print(f\"Number params in model: {count_parameters(model)}\")\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters())\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=20)\n",
        "\n",
        "best_val = float(\"inf\")\n",
        "logger = TrainResults(folder)\n",
        "total_it = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset [train]…\n",
            "  → 100 graphs\n",
            "Preprocessing: Computing graph partitions for 100 graphs...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "10it [00:50,  5.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Partitioned 10/100 graphs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "20it [01:39,  5.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Partitioned 20/100 graphs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "30it [02:30,  5.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Partitioned 30/100 graphs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "40it [03:21,  5.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Partitioned 40/100 graphs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "50it [04:11,  5.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Partitioned 50/100 graphs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "60it [05:01,  5.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Partitioned 60/100 graphs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "70it [05:52,  5.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Partitioned 70/100 graphs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "80it [06:41,  5.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Partitioned 80/100 graphs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "90it [07:33,  5.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Partitioned 90/100 graphs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100it [08:23,  5.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Partitioned 100/100 graphs\n",
            "Preprocessing completed in 503.67 seconds\n",
            "Average partitioning time: 5036.70ms per graph\n",
            "✔ Cached partitions to ./partition_cache/train_n10000_k10_partitions.pkl\n",
            "DataLoader ready!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#PARTITION\n",
        "# running time for partitioning: ~ 5-10 mins for 100 graphs\n",
        "\n",
        "load_from_cache = False  # set to false if no existing graph paritions cache\n",
        "\n",
        "# hyperparams manually written to match cache file (if used)\n",
        "n = 10000\n",
        "num_clusters = 20\n",
        "\n",
        "if load_from_cache: # load using params\n",
        "    train_loader = get_cluster_dataloader(\n",
        "        config[\"dataset\"], n, config[\"batch_size\"],\n",
        "        mode=\"train\", num_clusters=num_clusters,\n",
        "        clusters_per_batch=config[\"clusters_per_batch\"],\n",
        "        load_from_cache=load_from_cache\n",
        "    )\n",
        "\n",
        "else:  # run full graph parititioning\n",
        "    train_loader = get_cluster_dataloader(\n",
        "        config[\"dataset\"], config[\"n\"], config[\"batch_size\"],\n",
        "        mode=\"train\", num_clusters=config[\"num_clusters\"],\n",
        "        clusters_per_batch=config[\"clusters_per_batch\"],\n",
        "        load_from_cache=load_from_cache\n",
        "    )\n",
        "\n",
        "validation_loader = get_dataloader(config[\"dataset\"], config[\"n\"], 1, spd=(not gmres), mode=\"val\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "DnSu_43OXbsw"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100it [00:10,  9.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 \t loss: 511.6380316162109 \t time: 10.578984948000652\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100it [00:09, 10.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 \t loss: 246.5203454589844 \t time: 9.402286567999909\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100it [00:09, 10.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 \t loss: 137.14080520629884 \t time: 9.1661058589998\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100it [00:09, 10.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 \t loss: 126.32836334228516 \t time: 9.363024966998637\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100it [00:09, 10.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 \t loss: 121.8111898803711 \t time: 9.989059386000008\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100it [00:09, 11.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 \t loss: 116.57205749511719 \t time: 9.02541889999884\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100it [00:09, 10.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 \t loss: 113.5141293334961 \t time: 9.769512048000252\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100it [00:09, 10.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8 \t loss: 110.85219772338867 \t time: 9.441301711000051\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100it [00:09, 10.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9 \t loss: 107.37228118896485 \t time: 9.33072001599976\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100it [04:58,  2.98s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation\t iterations:\t702.40\n",
            "Epoch 10 \t loss: 104.30412284851074 \t time: 298.16499000099975\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100it [00:11,  8.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11 \t loss: 101.62905364990235 \t time: 11.434606515000269\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100it [00:11,  9.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12 \t loss: 100.26534980773926 \t time: 11.020935622000252\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100it [00:11,  9.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13 \t loss: 98.43288116455078 \t time: 11.011818689999927\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100it [00:11,  8.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14 \t loss: 97.81258888244629 \t time: 11.525848602001133\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100it [00:10,  9.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15 \t loss: 95.95898880004883 \t time: 10.86220885900002\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100it [00:11,  9.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16 \t loss: 95.70913780212402 \t time: 11.002918280999438\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100it [00:11,  8.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17 \t loss: 96.02757682800294 \t time: 11.316912579999553\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100it [00:11,  8.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18 \t loss: 92.39644554138184 \t time: 11.892047715000444\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100it [00:11,  8.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19 \t loss: 93.45857192993164 \t time: 11.674255378000453\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100it [03:52,  2.33s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation\t iterations:\t491.90\n",
            "Epoch 20 \t loss: 91.650072555542 \t time: 232.7134814789988\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100it [00:11,  8.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21 \t loss: 91.9603653717041 \t time: 11.343098290000853\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100it [00:10,  9.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22 \t loss: 94.18167778015138 \t time: 10.330789814999662\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "49it [00:05,  9.12it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m grad_norm = \u001b[32m0.0\u001b[39m\n\u001b[32m      7\u001b[39m start_epoch = time.perf_counter()\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m it, data \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(train_loader)):\n\u001b[32m     10\u001b[39m     total_it += \u001b[32m1\u001b[39m\n\u001b[32m     11\u001b[39m     model.train()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/GML_proj/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[32m   1182\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[32m   1183\u001b[39m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[32m   1184\u001b[39m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 208\u001b[39m, in \u001b[36mClusterGCNDataLoader.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    206\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[idx]\n\u001b[32m    207\u001b[39m     \u001b[38;5;66;03m# Sample clusters for this graph (partitioning already done)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m     subgraph_data = \u001b[38;5;28mself\u001b[39m.sampler.sample_clusters(data, \u001b[38;5;28mself\u001b[39m.clusters_per_batch)\n\u001b[32m    209\u001b[39m     batch_data.append(subgraph_data)\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batch_data) == \u001b[32m1\u001b[39m:\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 92\u001b[39m, in \u001b[36mClusterGCNSampler.sample_clusters\u001b[39m\u001b[34m(self, data, clusters_per_batch)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     89\u001b[39m \u001b[33;03mSample a subset of clusters and return the induced subgraph\u001b[39;00m\n\u001b[32m     90\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# Get or compute partitions (only done once per unique graph)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m node_clusters, cluster_to_nodes = \u001b[38;5;28mself\u001b[39m.partition_graph_once(data)\n\u001b[32m     94\u001b[39m \u001b[38;5;66;03m# Get available clusters (only non-empty ones)\u001b[39;00m\n\u001b[32m     95\u001b[39m available_clusters = \u001b[38;5;28mlist\u001b[39m(cluster_to_nodes.keys())\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mClusterGCNSampler.partition_graph_once\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpartition_graph_once\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[32m     40\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[33;03m    Partition the graph once and cache the results using stable graph hash\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     graph_hash = \u001b[38;5;28mself\u001b[39m._get_graph_hash(data)\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m graph_hash \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.partitions:\n\u001b[32m     46\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.partitions[graph_hash], \u001b[38;5;28mself\u001b[39m.cluster_nodes[graph_hash]\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mClusterGCNSampler._get_graph_hash\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[33;03mCreate a stable hash for the graph data that doesn't depend on object identity\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Create hash based on graph structure, not object identity\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m edge_hash = \u001b[38;5;28mhash\u001b[39m(\u001b[38;5;28mtuple\u001b[39m(data.edge_index.flatten().tolist()))\n\u001b[32m     25\u001b[39m node_hash = \u001b[38;5;28mhash\u001b[39m(data.num_nodes)\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Include matrix hash if present\u001b[39;00m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "#TRAIN\n",
        "\n",
        "# Train loop (similar to your existing code)\n",
        "for epoch in range(config[\"num_epochs\"]):\n",
        "    running_loss = 0.0\n",
        "    grad_norm = 0.0\n",
        "    start_epoch = time.perf_counter()\n",
        "\n",
        "    for it, data in tqdm(enumerate(train_loader)):\n",
        "        total_it += 1\n",
        "        model.train()\n",
        "\n",
        "        start = time.perf_counter()\n",
        "        data = data.to(device)\n",
        "\n",
        "        # Your existing training logic\n",
        "        output, reg, _ = model(data)\n",
        "        l = loss(output, data, c=reg, config=config[\"loss\"])\n",
        "\n",
        "        l.backward()\n",
        "        running_loss += l.item()\n",
        "\n",
        "        # Gradient handling (your existing code)\n",
        "        if \"gradient_clipping\" in config and config[\"gradient_clipping\"]:\n",
        "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config[\"gradient_clipping\"])\n",
        "        else:\n",
        "            total_norm = 0.0\n",
        "            for p in model.parameters():\n",
        "                if p.grad is not None:\n",
        "                    param_norm = p.grad.detach().data.norm(2)\n",
        "                    total_norm += param_norm.item() ** 2\n",
        "            grad_norm = total_norm ** 0.5 / config[\"batch_size\"]\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logger.log(l.item(), grad_norm, time.perf_counter() - start)\n",
        "\n",
        "        # Validation (your existing logic)\n",
        "        if (total_it) % 1000 == 0:\n",
        "            val_its = validate(model, validation_loader, solve=True, solver=\"cg\")\n",
        "            logger.log_val(None, val_its)\n",
        "\n",
        "            if val_its < best_val:\n",
        "                if config[\"save\"]:\n",
        "                    torch.save(model.state_dict(), f\"{folder}/best_model.pt\")\n",
        "                best_val = val_its\n",
        "\n",
        "    epoch_time = time.perf_counter() - start_epoch\n",
        "\n",
        "    if config[\"save\"]:\n",
        "        torch.save(model.state_dict(), f\"{folder}/model_epoch{epoch+1}.pt\")\n",
        "\n",
        "    print(f\"Epoch {epoch+1} \\t loss: {1/len(train_loader) * running_loss} \\t time: {epoch_time}\")\n",
        "\n",
        "# Final save\n",
        "if config[\"save\"]:\n",
        "    logger.save_results()\n",
        "    torch.save(model.to(torch.float).state_dict(), f\"{folder}/final_model.pt\")\n",
        "\n",
        "print(\"Best validation performance:\", best_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **DEBUGGING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9EwW-6nIfrC"
      },
      "outputs": [],
      "source": [
        "##DEBUGGING\n",
        "cluster_train_loader = get_cluster_dataloader(\n",
        "    config[\"dataset\"],\n",
        "    config[\"n\"],\n",
        "    config[\"batch_size\"],\n",
        "    spd=True,\n",
        "    mode=\"train\",\n",
        "    num_clusters=config[\"num_clusters\"],\n",
        "    clusters_per_batch=config[\"clusters_per_batch\"]\n",
        ")\n",
        "\n",
        "train_loader = get_dataloader(config[\"dataset\"], config[\"n\"], config[\"batch_size\"],\n",
        "                              spd=not gmres, mode=\"train\")\n",
        "\n",
        "print(cluster_train_loader)\n",
        "print(train_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i, data in enumerate(cluster_train_loader):\n",
        "  if i == 1:\n",
        "    print (data)\n",
        "for i, data in enumerate(train_loader):\n",
        "  if i == 1:\n",
        "    print (data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **TESTING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Using device: cpu\n",
            "Use model: NeuralIF\n",
            "Checkpoint not found...\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "neuralif.models.NeuralIF() argument after ** must be a mapping, not NoneType",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 319\u001b[39m\n\u001b[32m    315\u001b[39m     \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[32m    316\u001b[39m     test(model, testdata_loader, test_device, folder,\n\u001b[32m    317\u001b[39m          save_results=config[\u001b[33m\"\u001b[39m\u001b[33msave\u001b[39m\u001b[33m\"\u001b[39m], dataset=config[\u001b[33m\"\u001b[39m\u001b[33mdataset\u001b[39m\u001b[33m\"\u001b[39m], solver=config[\u001b[33m\"\u001b[39m\u001b[33msolver\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m call()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 307\u001b[39m, in \u001b[36mcall\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    304\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not available.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m     model = load_checkpoint(model, \u001b[38;5;28;01mNone\u001b[39;00m, test_device)\n\u001b[32m    308\u001b[39m     warmup(model, test_device)\n\u001b[32m    310\u001b[39m \u001b[38;5;66;03m# spd = config[\"solver\"] == \"cg\" or config[\"solver\"] == \"direct\"\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 218\u001b[39m, in \u001b[36mload_checkpoint\u001b[39m\u001b[34m(model, args, device)\u001b[39m\n\u001b[32m    211\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCheckpoint not found...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    213\u001b[39m \u001b[38;5;66;03m# neuralif has optional drop tolerance...\u001b[39;00m\n\u001b[32m    214\u001b[39m \u001b[38;5;66;03m# if args.model == \"neuralif\":\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;66;03m#     config[\"drop_tol\"] = args.drop_tol\u001b[39;00m\n\u001b[32m    216\u001b[39m \n\u001b[32m    217\u001b[39m \u001b[38;5;66;03m# intialize model and hyper-parameters\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m model = model(**config)\n\u001b[32m    219\u001b[39m checkpoint= \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m./results/experiment_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexperiment_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/best_model.pt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    220\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mload checkpoint: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mTypeError\u001b[39m: neuralif.models.NeuralIF() argument after ** must be a mapping, not NoneType"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "import numpy as np\n",
        "import scipy\n",
        "import scipy.sparse\n",
        "import torch\n",
        "import json\n",
        "\n",
        "from krylov.cg import conjugate_gradient, preconditioned_conjugate_gradient\n",
        "from krylov.gmres import gmres\n",
        "from krylov.preconditioner import get_preconditioner\n",
        "\n",
        "from neuralif.models import NeuralIF, NeuralPCG, PreCondNet, LearnedLU\n",
        "from neuralif.utils import torch_sparse_to_scipy, time_function\n",
        "from neuralif.logger import TestResults\n",
        "\n",
        "# from apps.data import matrix_to_graph_sparse, get_dataloader\n",
        "from apps.data import matrix_to_graph_sparse\n",
        "\n",
        "experiment_number = 6\n",
        "\n",
        "\n",
        "@torch.inference_mode()\n",
        "def test(model, test_loader, device, folder, save_results=False, dataset=\"random\", solver=\"cg\"):\n",
        "    \n",
        "    if save_results:\n",
        "        os.makedirs(folder, exist_ok=False)\n",
        "\n",
        "    print()\n",
        "    print(f\"Test:\\t{len(test_loader.dataset)} samples\")\n",
        "    print(f\"Solver:\\t{solver} solver\")\n",
        "    print()\n",
        "    \n",
        "    # Two modes: either test baselines or the learned preconditioner\n",
        "    if model is None:\n",
        "        methods = [\"baseline\", \"jacobi\", \"ilu\"]\n",
        "    else:\n",
        "        assert solver in [\"cg\", \"gmres\"], \"Data-driven method only works with CG or GMRES\"\n",
        "        methods = [\"learned\"]\n",
        "    \n",
        "    # using direct solver\n",
        "    if solver == \"direct\":\n",
        "        methods = [\"direct\"]\n",
        "    \n",
        "    for method in methods:\n",
        "        print(f\"Testing {method} preconditioner\")\n",
        "        \n",
        "        test_results = TestResults(method, dataset, folder,\n",
        "                                   model_name= f\"\\n{model.__class__.__name__}\" if method == \"learned\" else \"\",\n",
        "                                   target=1e-6,\n",
        "                                   solver=solver)\n",
        "        \n",
        "        for sample, data in enumerate(test_loader):\n",
        "            plot = save_results and sample == (len(test_loader.dataset) - 1)\n",
        "            \n",
        "            # Getting the preconditioners\n",
        "            start = time_function()\n",
        "            \n",
        "            data = data.to(device)\n",
        "            prec = get_preconditioner(data, method, model=model)\n",
        "            \n",
        "            # Get properties...\n",
        "            p_time = prec.time\n",
        "            breakdown = prec.breakdown\n",
        "            nnzL = prec.nnz\n",
        "            \n",
        "            stop = time_function()\n",
        "            \n",
        "            A = torch.sparse_coo_tensor(data.edge_index, data.edge_attr.squeeze(),\n",
        "                                        dtype=torch.float64,\n",
        "                                        requires_grad=False).to(\"cpu\").to_sparse_csr()\n",
        "            \n",
        "            b = data.x[:, 0].squeeze().to(\"cpu\").to(torch.float64)\n",
        "            b_norm = torch.linalg.norm(b)\n",
        "            \n",
        "            # we assume that b is unit norm wlog\n",
        "            b = b / b_norm\n",
        "            solution = data.s.to(\"cpu\").to(torch.float64).squeeze() / b_norm if hasattr(data, \"s\") else None\n",
        "            \n",
        "            overhead = (stop - start) - (p_time)\n",
        "            \n",
        "            # RUN CONJUGATE GRADIENT\n",
        "            start_solver = time_function()\n",
        "            \n",
        "            solver_settings = {\n",
        "                \"max_iter\": 10_000,\n",
        "                \"x0\": None\n",
        "            }\n",
        "            \n",
        "            if breakdown:\n",
        "                res = []\n",
        "            \n",
        "            elif solver == \"direct\":\n",
        "                \n",
        "                # convert to sparse matrix (scipy)\n",
        "                A_ = torch.sparse_coo_tensor(data.edge_index, data.edge_attr.squeeze(),\n",
        "                                             dtype=torch.float64, requires_grad=False)\n",
        "                \n",
        "                # scipy sparse...\n",
        "                A_s = torch_sparse_to_scipy(A_).tocsr()\n",
        "                \n",
        "                # override start time\n",
        "                start_solver = time_function()\n",
        "                \n",
        "                dense = False\n",
        "                \n",
        "                if dense:\n",
        "                    _ = scipy.linalg.solve(A_.to_dense().numpy(), b.numpy(), assume_a='pos')\n",
        "                else:\n",
        "                    _ = scipy.sparse.linalg.spsolve(A_s, b.numpy())\n",
        "                \n",
        "                # dummy values...\n",
        "                res = [(torch.Tensor([0]), torch.Tensor([0]))] * 2\n",
        "            \n",
        "            elif solver == \"cg\" and method == \"baseline\":\n",
        "                # no preconditioner required when using baseline method\n",
        "                res, _ = conjugate_gradient(A, b, x_true=solution,\n",
        "                                            rtol=test_results.target, **solver_settings)\n",
        "            \n",
        "            elif solver == \"cg\":\n",
        "                res, _ = preconditioned_conjugate_gradient(A, b, M=None, x_true=solution,\n",
        "                                                           rtol=test_results.target, **solver_settings)\n",
        "                \n",
        "            elif solver == \"gmres\":\n",
        "                \n",
        "                res, _ = gmres(A, b, M=prec, x_true=solution,\n",
        "                               **solver_settings, plot=plot,\n",
        "                               atol=test_results.target,\n",
        "                               left=False)\n",
        "            \n",
        "            stop_solver = time_function()\n",
        "            solver_time = (stop_solver - start_solver)\n",
        "            \n",
        "            # LOGGING\n",
        "            test_results.log_solve(A.shape[0], solver_time, len(res) - 1,\n",
        "                                   np.array([r[0].item() for r in res]),\n",
        "                                   np.array([r[1].item() for r in res]),\n",
        "                                   p_time, overhead)\n",
        "            \n",
        "            # ANALYSIS of the preconditioner and its effects!\n",
        "            nnzA = A._nnz()\n",
        "            \n",
        "            test_results.log(nnzA, nnzL, plot=plot)\n",
        "            \n",
        "            svd = False\n",
        "            if svd:\n",
        "                # compute largest and smallest singular value\n",
        "                Pinv = prec.get_inverse()\n",
        "                APinv = A.to_dense() @ Pinv\n",
        "                \n",
        "                # compute the singular values of the preconditioned matrix\n",
        "                S = torch.linalg.svdvals(APinv)\n",
        "                \n",
        "                # print the smallest and largest singular value\n",
        "                test_results.log_eigenval_dist(S, plot=plot)\n",
        "                \n",
        "                # compute the loss of the preconditioner\n",
        "                p = prec.get_p_matrix()\n",
        "                loss1 = torch.linalg.norm(p.to_dense() - A.to_dense(), ord=\"fro\")\n",
        "                \n",
        "                a_inv = torch.linalg.inv(A.to_dense())\n",
        "                loss2 = torch.linalg.norm(p.to_dense()@a_inv - torch.eye(a_inv.shape[0]), ord=\"fro\")\n",
        "                \n",
        "                test_results.log_loss(loss1, loss2, plot=False)\n",
        "                \n",
        "                print(f\"Smallest singular value: {S[-1]} | Largest singular value: {S[0]} | Condition number: {S[0] / S[-1]}\")\n",
        "                print(f\"Loss Lmax: {loss1}\\tLoss Lmin: {loss2}\")\n",
        "                print()\n",
        "                \n",
        "        if save_results:\n",
        "            test_results.save_results()\n",
        "        \n",
        "        test_results.print_summary()\n",
        "\n",
        "\n",
        "def load_checkpoint(model, args, device):\n",
        "    # load the saved weights of the model and the hyper-parameters\n",
        "    checkpoint = \"latest\"\n",
        "    \n",
        "    if checkpoint == \"latest\":\n",
        "        # list all the directories in the results folder\n",
        "        d = os.listdir(\"./results/\")\n",
        "        d.sort()\n",
        "        \n",
        "        config = None\n",
        "        \n",
        "        # find the latest checkpoint\n",
        "        for i in range(len(d)):\n",
        "            if os.path.isdir(\"./results/\" + d[-i-1]):\n",
        "                dir_contents = os.listdir(\"./results/\" + d[-i-1])\n",
        "                \n",
        "                # looking for a directory with both config and model weights\n",
        "                if \"config.json\" in dir_contents and \"final_model.pt\" in dir_contents:\n",
        "                    # load the config.json file\n",
        "                    with open(\"./results/\" + d[-i-1] + \"/config.json\") as f:\n",
        "                        config = json.load(f)\n",
        "                        \n",
        "                        # if config[\"model\"] != args.model:\n",
        "                        #     config = None\n",
        "                        #     continue\n",
        "                        #There is currently no best model yet\n",
        "                        if \"best_model.pt\" in dir_contents:\n",
        "                            checkpoint = \"./results/\" + d[-i-1] + \"/best_model.pt\"\n",
        "                            break\n",
        "                        else:\n",
        "                            checkpoint = \"./results/\" + d[-i-1] + \"/final_model.pt\"\n",
        "                            break\n",
        "        if config is None:\n",
        "            print(\"Checkpoint not found...\")\n",
        "        \n",
        "        # neuralif has optional drop tolerance...\n",
        "        # if args.model == \"neuralif\":\n",
        "        #     config[\"drop_tol\"] = args.drop_tol\n",
        "        \n",
        "        # intialize model and hyper-parameters\n",
        "        model = model(**config)\n",
        "        checkpoint= f\"./results/experiment_{experiment_number}/best_model.pt\"\n",
        "        print(f\"load checkpoint: {checkpoint}\")\n",
        "        \n",
        "        model.load_state_dict(torch.load(checkpoint, weights_only=False, map_location=torch.device(device)))\n",
        "    \n",
        "    elif checkpoint is not None:\n",
        "        with open(checkpoint + \"/config.json\") as f:\n",
        "            config = json.load(f)\n",
        "        \n",
        "        # if args.model == \"neuralif\":\n",
        "        #     config[\"drop_tol\"] = args.drop_tol\n",
        "        \n",
        "        model = model(**config)\n",
        "        print(f\"load checkpoint: {checkpoint}\")\n",
        "        model.load_state_dict(torch.load(checkpoint + f\"/{args.weights}.pt\",\n",
        "                                            map_location=torch.device(model.device)))\n",
        "    \n",
        "    else:\n",
        "        model = model(**{\"global_features\": 0, \"latent_size\": 8, \"augment_nodes\": False,\n",
        "                            \"message_passing_steps\": 3, \"skip_connections\": True, \"activation\": \"relu\",\n",
        "                            \"aggregate\": None, \"decode_nodes\": False})\n",
        "        \n",
        "        print(\"No checkpoint provided, using random weights\")\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "def warmup(model, device):\n",
        "    # set testing parameters\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    # run model warmup\n",
        "    test_size = 1_000\n",
        "    matrix = scipy.sparse.coo_matrix((np.ones(test_size), (np.arange(test_size), np.arange(test_size))))\n",
        "    data = matrix_to_graph_sparse(matrix, torch.ones(test_size))\n",
        "    data.to(device)\n",
        "    _ = model(data)\n",
        "    \n",
        "    print(\"Model warmup done...\")\n",
        "\n",
        "\n",
        "# # argument is the model to load and the dataset to evaluate on\n",
        "# def argparser():\n",
        "#     parser = argparse.ArgumentParser()\n",
        "\n",
        "#     parser.add_argument(\"--name\", type=str, default=None)\n",
        "#     parser.add_argument(\"--device\", type=int, required=False)\n",
        "    \n",
        "#     # select data driven model to run\n",
        "#     parser.add_argument(\"--model\", type=str, required=False, default=\"none\")\n",
        "#     parser.add_argument(\"--checkpoint\", type=str, required=False)\n",
        "#     parser.add_argument(\"--weights\", type=str, required=False, default=\"model\")\n",
        "#     parser.add_argument(\"--drop_tol\", type=float, default=0)\n",
        "    \n",
        "#     parser.add_argument(\"--solver\", type=str, default=\"cg\")\n",
        "    \n",
        "#     # select dataset and subset\n",
        "#     parser.add_argument(\"--dataset\", type=str, required=False, default=\"random\")\n",
        "#     parser.add_argument(\"--subset\", type=str, required=False, default=\"test\")\n",
        "#     parser.add_argument(\"--n\", type=int, required=False, default=0)\n",
        "#     parser.add_argument(\"--samples\", type=int, required=False, default=None)\n",
        "    \n",
        "#     # select if to save\n",
        "#     parser.add_argument(\"--save\", action='store_true', default=False)\n",
        "    \n",
        "#     return parser.parse_args()\n",
        "\n",
        "\n",
        "def call():\n",
        "    \n",
        "    test_device = \"cpu\"\n",
        "        \n",
        "    folder = folder = \"results/\" + datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
        "    \n",
        "    print()\n",
        "    print(f\"Using device: {test_device}\")\n",
        "    # torch.set_num_threads(1)\n",
        "    \n",
        "    # Load the model\n",
        "    if config[\"model\"] == \"nif\" or config[\"model\"] == \"neuralif\":\n",
        "        print(\"Use model: NeuralIF\")\n",
        "        model = NeuralIF\n",
        "    \n",
        "    else:\n",
        "        raise NotImplementedError(f\"Model {config[\"model\"]} not available.\")\n",
        "    \n",
        "    if model is not None:\n",
        "        model = load_checkpoint(model, None, test_device)\n",
        "        warmup(model, test_device)\n",
        "    \n",
        "    # spd = config[\"solver\"] == \"cg\" or config[\"solver\"] == \"direct\"\n",
        "    spd = True\n",
        "    testdata_loader = get_dataloader(config[\"dataset\"], n=config[\"n\"], batch_size=1, mode=\"test\",\n",
        "                                     size=config[\"samples\"], spd=spd, graph=True)\n",
        "    \n",
        "    # Evaluate the model\n",
        "    test(model, testdata_loader, test_device, folder,\n",
        "         save_results=config[\"save\"], dataset=config[\"dataset\"], solver=config[\"solver\"])\n",
        "\n",
        "call()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "GML_proj",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
