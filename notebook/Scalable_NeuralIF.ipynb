{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Scalable GNN–Based Preconditioners for Conjugate Gradient Methods\n",
        "**Authors: Nicholas Tan Yun Yu, Low Jun Yu, Yuhan Wu**\n",
        "\n",
        "This project was inspired by [NeuralIF](https://arxiv.org/abs/2305.16368).\n",
        "\n",
        "**Summary**: The authors come up with a novel message-passing GNN block that is used by the network to predict efficient preconditioners to solve sparse linear systems. These preconditioners are tested using the preconditioned conjugate gradient (CG) method, which make the algorithm converge faster than other state-of-the-art preconditioners.\n",
        "\n",
        "**Motivation**: Modern data-driven and physics-based applications frequently force us to deal with dense matrices. Therefore, we hope to show that the message-passing GNN block can learn effective preconditioners for these scaled up fields. An example of a machine learning problem that could benefit from this is Gaussian Processes, which makes use of a dense kernel function as such: (some image)\n",
        "\n",
        "**The problem**: Scaling the problem to dense matrices is nontrivial. The Coates graph representation has 1 node per row/column, and one edge only for each nonzero entry in A. For a dense n*n matrix, that graph becomes complete – with n^2 edges – so both memory and compute blow up to O(n^2).\n",
        "\n",
        "**Research direction**: Implement an edge-regression GNN that can work on dense matrices. We can achieve this using sampling techniques such as GraphSAGE and Cluster-GCN."
      ],
      "metadata": {
        "id": "J_CMa5FedgDR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Installation & Setup"
      ],
      "metadata": {
        "id": "XrqVh2BF7LhH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load files from google drive"
      ],
      "metadata": {
        "id": "-4MVBD1eXd8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSuii7-phJYZ",
        "outputId": "ad8ba5ae-8402-4844-d912-7e87e34006db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add directory to python's path\n",
        "# This directory path should lead straight to the root of the project\n",
        "# Ensure that the project's root directory has the folders \"krylov\", \"apps\" and \"neuralif\"\n",
        "#   that has the files contained in https://github.com/paulhausner/neural-incomplete-factorization/tree/main\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/CS4350/project/')"
      ],
      "metadata": {
        "id": "tbuq9axkgIj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Package installation"
      ],
      "metadata": {
        "id": "ehpIKqQaX36-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q torch_geometric"
      ],
      "metadata": {
        "id": "WuLYpuGUDMoA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ea6be76-d3ce-4073-835d-bfbf0611f5e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "HgiuVFCFX9Ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import datetime\n",
        "import pprint\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import scipy\n",
        "import torch\n",
        "import torch_geometric\n",
        "import torch.nn as nn\n",
        "import torch_geometric.nn as pyg\n",
        "from torch_geometric.nn import aggr\n",
        "from torch_geometric.utils import to_scipy_sparse_matrix\n",
        "from scipy.sparse import tril, coo_matrix\n",
        "\n",
        "from apps.data import get_dataloader, graph_to_matrix, matrix_to_graph\n",
        "from neuralif.utils import (\n",
        "    count_parameters, save_dict_to_file,\n",
        "    condition_number, eigenval_distribution, gershgorin_norm\n",
        ")\n",
        "from neuralif.logger import TrainResults, TestResults\n",
        "from neuralif.loss import loss\n",
        "# from neuralif.models import NeuralPCG, NeuralIF, PreCondNet, LearnedLU\n",
        "\n",
        "from krylov.cg import preconditioned_conjugate_gradient\n",
        "from krylov.gmres import gmres\n",
        "\n",
        "# import from self-curated numml file\n",
        "from numml import SparseCSRTensor"
      ],
      "metadata": {
        "id": "c3uK7LjaQoMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set GPU"
      ],
      "metadata": {
        "id": "1BzKAAbyYE59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDfXtM3xQzwJ",
        "outputId": "6d930c4e-22e8-42f6-d795-641f8184f53e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Dataset generation"
      ],
      "metadata": {
        "id": "zmYSP8cAZFaG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper functions"
      ],
      "metadata": {
        "id": "cA9OMUsXaQKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sparse_random(n, alpha=1e-4, random_state=0, sol=False, ood=False):\n",
        "    rng = np.random.RandomState(random_state)\n",
        "    if alpha is None:\n",
        "        alpha = rng.uniform(1e-4, 1e-2)\n",
        "    sparsity = 10e-4  # this is 1% sparsity for n = 10 000\n",
        "\n",
        "    if ood:\n",
        "        factor = rng.uniform(0.22, 2.2)\n",
        "        sparsity *= factor\n",
        "\n",
        "    nnz = int(sparsity * n ** 2)\n",
        "    rows = [rng.randint(0, n) for _ in range(nnz)]\n",
        "    cols = [rng.randint(0, n) for _ in range(nnz)]\n",
        "    uniques = set(zip(rows, cols))\n",
        "    rows, cols = zip(*uniques)\n",
        "    vals = rng.normal(0, 1, size=len(cols))\n",
        "\n",
        "    M = coo_matrix((vals, (rows, cols)), shape=(n, n))\n",
        "    I = scipy.sparse.identity(n)\n",
        "    A = (M @ M.T) + alpha * I # create spd matrix\n",
        "    print(f\"Generated matrix with {100 * (A.nnz / n**2):.2f}% non-zeros ({A.nnz} entries)\")\n",
        "\n",
        "    b = rng.uniform(0, 1, size=n)\n",
        "    x = None\n",
        "    if sol:\n",
        "        x, _ = scipy.sparse.linalg.cg(A, b)\n",
        "    return A, x, b\n",
        "\n",
        "def create_dataset(n, samples, alpha=1e-2, graph=True, rs=0, mode='train', solution=False):\n",
        "    if mode != 'train' and rs == 0:\n",
        "        raise ValueError('`rs` must be non-zero for val/test to avoid overlap')\n",
        "\n",
        "    print(f\"Creating {samples} samples for '{mode}' set (n={n})\")\n",
        "    for sam in range(samples):\n",
        "        A, x, b = generate_sparse_random(\n",
        "            n, alpha=alpha, random_state=rs + sam,\n",
        "            sol=solution, ood=(mode == \"test_ood\")\n",
        "        )\n",
        "        if graph:\n",
        "            g = matrix_to_graph(A, b)\n",
        "            if x is not None:\n",
        "                g.s = torch.tensor(x, dtype=torch.float)\n",
        "            g.n = n\n",
        "            torch.save(g, f'./data/Random/{mode}/{n}_{sam}.pt')\n",
        "        else:\n",
        "            scipy.sparse.save_npz(f'./data/Random/{mode}/{n}_{sam}.npz', A)\n",
        "            np.savez(f'./data/Random/{mode}/{n}_{sam}.npz', A=A, b=b, x=x)\n"
      ],
      "metadata": {
        "id": "JKMq525PZEId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Train, Validation and Test datasets"
      ],
      "metadata": {
        "id": "2Coq9OGHdUm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ensure target folders exist\n",
        "for split in ['train', 'val', 'test']:\n",
        "    os.makedirs(f'./data/Random/{split}', exist_ok=True)\n",
        "\n",
        "# parameters\n",
        "n = 10_000\n",
        "alpha = 10e-4\n",
        "\n",
        "# generate\n",
        "create_dataset(n, samples=1000, alpha=alpha, mode='train', rs=0, graph=True, solution=True)\n",
        "create_dataset(n, samples=10, alpha=alpha, mode='val', rs=10000, graph=True, solution=False)\n",
        "create_dataset(n, samples=100, alpha=alpha, mode='test', rs=103600, graph=True, solution=False)\n"
      ],
      "metadata": {
        "id": "lWJ7E9ZEaCQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Models"
      ],
      "metadata": {
        "id": "4Jmnl78bbJzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralIF(nn.Module):\n",
        "    # Neural Incomplete factorization\n",
        "    def __init__(self, drop_tol=0, **kwargs) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.global_features = kwargs[\"global_features\"]\n",
        "        self.latent_size = kwargs[\"latent_size\"]\n",
        "        # node features are augmented with local degree profile\n",
        "        self.augment_node_features = kwargs[\"augment_nodes\"]\n",
        "\n",
        "        num_node_features = 8 if self.augment_node_features else 1\n",
        "        message_passing_steps = kwargs[\"message_passing_steps\"]\n",
        "\n",
        "        # edge feature representation in the latent layers\n",
        "        edge_features = kwargs.get(\"edge_features\", 1)\n",
        "\n",
        "        self.skip_connections = kwargs[\"skip_connections\"]\n",
        "\n",
        "        self.mps = torch.nn.ModuleList()\n",
        "        for l in range(message_passing_steps):\n",
        "            # skip connections are added to all layers except the first one\n",
        "            self.mps.append(MP_Block(skip_connections=self.skip_connections,\n",
        "                                     first=l==0,\n",
        "                                     last=l==(message_passing_steps-1),\n",
        "                                     edge_features=edge_features,\n",
        "                                     node_features=num_node_features,\n",
        "                                     global_features=self.global_features,\n",
        "                                     hidden_size=self.latent_size,\n",
        "                                     activation=kwargs[\"activation\"],\n",
        "                                     aggregate=kwargs[\"aggregate\"]))\n",
        "\n",
        "        # node decodings\n",
        "        self.node_decoder = MLP([num_node_features, self.latent_size, 1]) if kwargs[\"decode_nodes\"] else None\n",
        "\n",
        "        # diag-aggregation for normalization of rows\n",
        "        self.normalize_diag = kwargs[\"normalize_diag\"] if \"normalize_diag\" in kwargs else False\n",
        "        self.diag_aggregate = aggr.SumAggregation()\n",
        "\n",
        "        # normalization\n",
        "        self.graph_norm = pyg.norm.GraphNorm(num_node_features) if (\"graph_norm\" in kwargs and kwargs[\"graph_norm\"]) else None\n",
        "\n",
        "        # drop tolerance and additional fill-ins and more sparsity\n",
        "        self.tau = drop_tol\n",
        "        self.two = kwargs.get(\"two_hop\", False)\n",
        "\n",
        "    def forward(self, data):\n",
        "        # ! data could be batched here...(not implemented)\n",
        "\n",
        "        if self.augment_node_features:\n",
        "            data = augment_features(data, skip_rhs=True)\n",
        "\n",
        "        # add additional edges to the data\n",
        "        if self.two:\n",
        "            data = TwoHop()(data)\n",
        "\n",
        "        # * in principle it is possible to integrate reordering here.\n",
        "\n",
        "        data = ToLowerTriangular()(data)\n",
        "\n",
        "        # get the input data\n",
        "        edge_embedding = data.edge_attr\n",
        "        l_index = data.edge_index\n",
        "\n",
        "        if self.graph_norm is not None:\n",
        "            node_embedding = self.graph_norm(data.x, batch=data.batch)\n",
        "        else:\n",
        "            node_embedding = data.x\n",
        "\n",
        "        # copy the input data (only edges of original matrix A)\n",
        "        a_edges = edge_embedding.clone()\n",
        "\n",
        "        if self.global_features > 0:\n",
        "            global_features = torch.zeros((1, self.global_features), device=data.x.device, requires_grad=False)\n",
        "            # feature ideas: nnz, 1-norm, inf-norm col/row var, min/max variability, avg distances to nnz\n",
        "        else:\n",
        "            global_features = None\n",
        "\n",
        "        # compute the output of the network\n",
        "        for i, layer in enumerate(self.mps):\n",
        "            if i != 0 and self.skip_connections:\n",
        "                edge_embedding = torch.cat([edge_embedding, a_edges], dim=1)\n",
        "\n",
        "            edge_embedding, node_embedding, global_features = layer(node_embedding, l_index, edge_embedding, global_features)\n",
        "\n",
        "        # transform the output into a matrix\n",
        "        return self.transform_output_matrix(node_embedding, l_index, edge_embedding, a_edges)\n",
        "\n",
        "    def transform_output_matrix(self, node_x, edge_index, edge_values, a_edges):\n",
        "        # force diagonal to be positive\n",
        "        diag = edge_index[0] == edge_index[1]\n",
        "\n",
        "        # normalize diag such that it has zero residual\n",
        "        if self.normalize_diag:\n",
        "            # copy the diag of matrix A\n",
        "            a_diag = a_edges[diag]\n",
        "\n",
        "            # compute the row norm\n",
        "            square_values = torch.pow(edge_values, 2)\n",
        "            aggregated = self.diag_aggregate(square_values, edge_index[0])\n",
        "\n",
        "            # now, we renormalize the edge values such that they are the square root of the original value...\n",
        "            edge_values = torch.sqrt(a_diag[edge_index[0]]) * edge_values / torch.sqrt(aggregated[edge_index[0]])\n",
        "\n",
        "        else:\n",
        "            # otherwise, just take the edge values as they are...\n",
        "            # but take the square root as it is numerically better\n",
        "            # edge_values[diag] = torch.exp(edge_values[diag])\n",
        "            edge_values[diag] = torch.sqrt(torch.exp(edge_values[diag]))\n",
        "\n",
        "        # node decoder\n",
        "        node_output = self.node_decoder(node_x).squeeze() if self.node_decoder is not None else None\n",
        "\n",
        "        # ! this if should only be activated when the model is in production!!\n",
        "        if torch.is_inference_mode_enabled():\n",
        "\n",
        "            # we can decide to remove small elements during inference from the preconditioner matrix\n",
        "            if self.tau != 0:\n",
        "                small_value = (torch.abs(edge_values) <= self.tau).squeeze()\n",
        "\n",
        "                # small value and not diagonal\n",
        "                elems = torch.logical_and(small_value, torch.logical_not(diag))\n",
        "\n",
        "                # might be able to do this easily!\n",
        "                edge_values[elems] = 0\n",
        "\n",
        "                # remove zeros from the sparse representation\n",
        "                filt = (edge_values != 0).squeeze()\n",
        "                edge_values = edge_values[filt]\n",
        "                edge_index = edge_index[:, filt]\n",
        "\n",
        "            # ! this is the way to go!!\n",
        "            # Doing pytorch -> scipy -> numml is a lot faster than pytorch -> numml on CPU\n",
        "            # On GPU it is faster to go to pytorch -> numml -> CPU\n",
        "\n",
        "            # convert to scipy sparse matrix\n",
        "            # m = to_scipy_sparse_matrix(edge_index, matrix_values)\n",
        "            m = torch.sparse_coo_tensor(edge_index, edge_values.squeeze(),\n",
        "                                        size=(node_x.size()[0], node_x.size()[0]))\n",
        "                                        # type=torch.double)\n",
        "\n",
        "            # produce L and U seperatly\n",
        "            l = sp.SparseCSRTensor(m)\n",
        "            u = sp.SparseCSRTensor(m.T)\n",
        "\n",
        "            return l, u, node_output\n",
        "\n",
        "        else:\n",
        "            # For training and testing (computing regular losses for examples.)\n",
        "            # does not need to be performance optimized!\n",
        "            # use torch sparse directly\n",
        "            t = torch.sparse_coo_tensor(edge_index, edge_values.squeeze(),\n",
        "                                        size=(node_x.size()[0], node_x.size()[0]))\n",
        "\n",
        "            # normalized l1 norm is best computed here!\n",
        "            # l2_nn = torch.linalg.norm(edge_values, ord=2)\n",
        "            l1_penalty = torch.sum(torch.abs(edge_values)) / len(edge_values)\n",
        "\n",
        "            return t, l1_penalty, node_output\n"
      ],
      "metadata": {
        "id": "fo19YfVkW3cT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Training"
      ],
      "metadata": {
        "id": "C_ySuevTbPWA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Training Configuration"
      ],
      "metadata": {
        "id": "p9Rge5QlbVmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"name\": \"experiment_1\",\n",
        "    \"save\": True,\n",
        "    \"seed\": 42,\n",
        "    \"n\": 0,\n",
        "    \"batch_size\": 1,\n",
        "    \"num_epochs\": 100,\n",
        "    \"dataset\": \"random\",\n",
        "    \"loss\": \"frobenius\",\n",
        "    \"gradient_clipping\": 1.0,\n",
        "    \"regularizer\": 0.0,\n",
        "    \"scheduler\": False,\n",
        "    \"model\": \"neuralif\",\n",
        "    \"normalize\": False,\n",
        "    \"latent_size\": 8,\n",
        "    \"message_passing_steps\": 3,\n",
        "    \"decode_nodes\": False,\n",
        "    \"normalize_diag\": False,\n",
        "    \"aggregate\": [],\n",
        "    \"activation\": \"relu\",\n",
        "    \"skip_connections\": True,\n",
        "    \"augment_nodes\": False,\n",
        "    \"global_features\": 0,\n",
        "    \"edge_features\": 1,\n",
        "    \"graph_norm\": False,\n",
        "    \"two_hop\": False,\n",
        "}\n",
        "\n",
        "# Prepare output folder\n",
        "if config[\"name\"]:\n",
        "    folder = f\"results/{config['name']}\"\n",
        "else:\n",
        "    folder = datetime.datetime.now().strftime(\"results/%Y-%m-%d_%H-%M-%S\")\n",
        "if config[\"save\"]:\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "    save_dict_to_file(config, os.path.join(folder, \"config.json\"))\n"
      ],
      "metadata": {
        "id": "Og3JzHyyQ5-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seed for reproducibility\n",
        "torch_geometric.seed_everything(config[\"seed\"])\n",
        "\n",
        "# Select model\n",
        "model_args = {k: config[k] for k in [\n",
        "    \"latent_size\", \"message_passing_steps\", \"skip_connections\",\n",
        "    \"augment_nodes\", \"global_features\", \"decode_nodes\",\n",
        "    \"normalize_diag\", \"activation\", \"aggregate\", \"graph_norm\",\n",
        "    \"two_hop\", \"edge_features\", \"normalize\"\n",
        "] if k in config}\n",
        "\n",
        "use_gmres = False\n",
        "if config[\"model\"] == \"__\":\n",
        "    model = NeuralPCG(**model_args)\n",
        "elif config[\"model\"] in (\"nif\", \"neuralif\", \"inf\"):\n",
        "    model = NeuralIF(**model_args)\n",
        "else:\n",
        "    raise ValueError(\"Unknown model type\")\n",
        "\n",
        "model.to(device)\n",
        "print(\"Number of parameters:\", count_parameters(model))\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters())\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode=\"min\", factor=0.5, patience=20\n",
        ")"
      ],
      "metadata": {
        "id": "tJnpsDJBRzJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# helper functions\n",
        "@torch.no_grad()\n",
        "def validate(model, validation_loader, solve=False, solver=\"cg\"):\n",
        "    model.eval()\n",
        "    acc_loss = 0.0\n",
        "    num_loss = 0\n",
        "    acc_solver_iters = 0.0\n",
        "\n",
        "    for data in validation_loader:\n",
        "        data = data.to(device)\n",
        "        A, b = graph_to_matrix(data)\n",
        "\n",
        "        if solve:\n",
        "            preconditioner = LearnedPreconditioner(data, model)\n",
        "            A_cpu = A.cpu().double()\n",
        "            b_cpu = b.cpu().double()\n",
        "            x0 = None\n",
        "\n",
        "            start = time.time()\n",
        "            if solver == \"cg\":\n",
        "                iters, x_hat = preconditioned_conjugate_gradient(\n",
        "                    A_cpu, b_cpu, M=preconditioner, x0=x0,\n",
        "                    rtol=1e-6, max_iter=1000\n",
        "                )\n",
        "            else:\n",
        "                iters, x_hat = gmres(\n",
        "                    A_cpu, b_cpu, M=preconditioner, x0=x0,\n",
        "                    atol=1e-6, max_iter=1000, left=False\n",
        "                )\n",
        "            acc_solver_iters += len(iters) - 1\n",
        "        else:\n",
        "            output, _, _ = model(data)\n",
        "            # l = frobenius_loss(output, A)\n",
        "            l = loss(data, output, config=\"frobenius\")\n",
        "            acc_loss += l.item()\n",
        "            num_loss += 1\n",
        "\n",
        "    if solve:\n",
        "        avg_iters = acc_solver_iters / len(validation_loader)\n",
        "        print(f\"Validation iterations: {avg_iters:.2f}\")\n",
        "        return avg_iters\n",
        "    else:\n",
        "        avg_loss = acc_loss / num_loss\n",
        "        print(f\"Validation loss: {avg_loss:.4f}\")\n",
        "        return avg_loss"
      ],
      "metadata": {
        "id": "bRETenOFREJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "best_val = float(\"inf\")\n",
        "logger = TrainResults(folder)\n",
        "total_it = 0\n",
        "\n",
        "for epoch in range(config[\"num_epochs\"]):\n",
        "    running_loss = 0.0\n",
        "    start_epoch = time.perf_counter()\n",
        "\n",
        "    for data in train_loader:\n",
        "        total_it += 1\n",
        "        model.train()\n",
        "        data = data.to(device)\n",
        "\n",
        "        output, reg, _ = model(data)\n",
        "        l = loss(output, data, c=reg, config=config[\"loss\"])\n",
        "        l.backward()\n",
        "\n",
        "        # gradient clipping or manual norm\n",
        "        if config[\"gradient_clipping\"]:\n",
        "            grad_norm = torch.nn.utils.clip_grad_norm_(\n",
        "                model.parameters(), config[\"gradient_clipping\"]\n",
        "            )\n",
        "        else:\n",
        "            total_norm = sum(\n",
        "                p.grad.detach().data.norm(2).item() ** 2\n",
        "                for p in model.parameters() if p.grad is not None\n",
        "            )\n",
        "            grad_norm = (total_norm ** 0.5) / config[\"batch_size\"]\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        running_loss += l.item()\n",
        "        logger.log(l.item(), grad_norm, time.perf_counter() - start=0)\n",
        "\n",
        "        # periodic validation\n",
        "        if total_it % 1000 == 0:\n",
        "            val_metric = validate(\n",
        "                model, val_loader, solve=True,\n",
        "                solver=\"gmres\" if use_gmres else \"cg\"\n",
        "            )\n",
        "            logger.log_val(None, val_metric)\n",
        "            if val_metric < best_val:\n",
        "                best_val = val_metric\n",
        "                if config[\"save\"]:\n",
        "                    torch.save(model.state_dict(), f\"{folder}/best_model.pt\")\n",
        "\n",
        "    epoch_time = time.perf_counter() - start_epoch\n",
        "    print(f\"Epoch {epoch+1} — loss: {running_loss/len(train_loader):.4f}, time: {epoch_time:.1f}s\")\n",
        "    if config[\"save\"]:\n",
        "        torch.save(model.state_dict(), f\"{folder}/model_epoch{epoch+1}.pt\")\n"
      ],
      "metadata": {
        "id": "sMSaRR7cSMFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save results\n",
        "if config[\"save\"]:\n",
        "    logger.save_results()\n",
        "    torch.save(model.to(torch.float).state_dict(), f\"{folder}/final_model.pt\")\n",
        "\n"
      ],
      "metadata": {
        "id": "vom7sud2SSpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test printout\n",
        "print(\"Best validation performance:\", best_val)"
      ],
      "metadata": {
        "id": "5JqtkgTZSaSG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}